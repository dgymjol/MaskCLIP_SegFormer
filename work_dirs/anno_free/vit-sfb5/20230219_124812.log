2023-02-19 12:48:13,020 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.16 (default, Jan 17 2023, 23:13:24) [GCC 11.2.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 3090
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.2, V11.2.152
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.9.1+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.10.1+cu111
OpenCV: 4.7.0
MMCV: 1.5.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
MMSegmentation: 0.20.2+e20a1c2
------------------------------------------------------------

2023-02-19 12:48:13,021 - mmseg - INFO - Distributed training: False
2023-02-19 12:48:14,645 - mmseg - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='EncoderDecoder',
    pretrained='pretrain/mit_b5_weight.pth',
    backbone=dict(
        type='MixVisionTransformer',
        in_channels=3,
        embed_dims=64,
        num_stages=4,
        num_layers=[3, 6, 40, 3],
        num_heads=[1, 2, 5, 8],
        patch_sizes=[7, 3, 3, 3],
        sr_ratios=[8, 4, 2, 1],
        out_indices=(0, 1, 2, 3),
        mlp_ratio=4,
        qkv_bias=True,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.1),
    decode_head=dict(
        type='MaskClipPlusSegformerHead',
        vit=True,
        in_channels=2048,
        channels=512,
        num_classes=59,
        dropout_ratio=0,
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
        decode_module_cfg=dict(
            type='SegformerHead',
            in_channels=[64, 128, 320, 512],
            in_index=[0, 1, 2, 3],
            channels=512,
            dropout_ratio=0.1,
            num_classes=59,
            norm_cfg=dict(type='SyncBN', requires_grad=True),
            align_corners=False),
        text_categories=59,
        text_channels=512,
        clip_channels=768,
        text_embeddings_path='pretrain/context_ViT16_clip_text.pth',
        cls_bg=False,
        norm_feat=False,
        clip_unlabeled_cats=[
            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,
            19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
            36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52,
            53, 54, 55, 56, 57, 58
        ],
        clip_cfg=dict(
            type='VisionTransformer',
            img_size=(224, 224),
            patch_size=16,
            patch_bias=False,
            in_channels=3,
            embed_dims=768,
            num_layers=12,
            num_heads=12,
            mlp_ratio=4,
            out_indices=-1,
            qkv_bias=True,
            drop_rate=0.0,
            attn_drop_rate=0.0,
            drop_path_rate=0.0,
            with_cls_token=True,
            output_cls_token=False,
            norm_cfg=dict(type='LN', eps=1e-06),
            act_cfg=dict(type='GELU'),
            patch_norm=False,
            pre_norm=True,
            final_norm=True,
            return_qkv=True,
            interpolate_mode='bicubic',
            num_fcs=2,
            norm_eval=False),
        clip_weights_path='pretrain/ViT16_clip_weights.pth',
        reset_counter=True,
        start_clip_guided=(1, -1),
        start_self_train=(-1, -1)),
    feed_img_to_decode_head=True,
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))
dataset_type = 'PascalContextDataset59'
data_root = 'data/VOCdevkit/VOC2010/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
img_scale = (520, 520)
crop_size = (480, 480)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='LoadAnnotations',
        reduce_zero_label=True,
        suppress_labels=[
            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,
            19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
            36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52,
            53, 54, 55, 56, 57, 58
        ]),
    dict(type='Resize', img_scale=(520, 520), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(480, 480), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(480, 480), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(520, 520),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=8,
    workers_per_gpu=4,
    train=dict(
        type='PascalContextDataset59',
        data_root='data/VOCdevkit/VOC2010/',
        img_dir='JPEGImages',
        ann_dir='SegmentationClassContext',
        split='ImageSets/SegmentationContext/train.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='LoadAnnotations',
                reduce_zero_label=True,
                suppress_labels=[
                    0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
                    17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,
                    32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46,
                    47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58
                ]),
            dict(type='Resize', img_scale=(520, 520), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(480, 480), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(480, 480), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ]),
    val=dict(
        type='PascalContextDataset59',
        data_root='data/VOCdevkit/VOC2010/',
        img_dir='JPEGImages',
        ann_dir='SegmentationClassContext',
        split='ImageSets/SegmentationContext/val.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(520, 520),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='PascalContextDataset59',
        data_root='data/VOCdevkit/VOC2010/',
        img_dir='JPEGImages',
        ann_dir='SegmentationClassContext',
        split='ImageSets/SegmentationContext/val.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(520, 520),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
optimizer = dict(
    type='AdamW',
    lr=2.4e-05,
    betas=(0.9, 0.999),
    weight_decay=0.004,
    paramwise_cfg=dict(
        custom_keys=dict(
            pos_block=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0),
            head=dict(lr_mult=10.0))))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=1500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=8000)
checkpoint_config = dict(by_epoch=False, interval=4000)
evaluation = dict(interval=4000, metric='mIoU', pre_eval=True)
suppress_labels = [
    0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,
    21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,
    40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58
]
find_unused_parameters = True
work_dir = 'work_dirs/anno_free/vit-sfb5'
gpu_ids = range(0, 1)
auto_resume = False

2023-02-19 12:48:14,646 - mmseg - INFO - Set random seed to 1826304042, deterministic: False
2023-02-19 12:48:20,105 - mmseg - INFO - initialize MixVisionTransformer with init_cfg {'type': 'Pretrained', 'checkpoint': 'pretrain/mit_b5_weight.pth'}
2023-02-19 12:48:21,706 - mmseg - INFO - initialize SegformerHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
2023-02-19 12:48:23,910 - mmseg - INFO - Loaded text embeddings from pretrain/context_ViT16_clip_text.pth
2023-02-19 12:48:24,185 - mmseg - INFO - Loaded clip weights from pretrain/ViT16_clip_weights.pth
Name of parameter - Initialization information

backbone.layers.0.0.projection.weight - torch.Size([64, 3, 7, 7]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.0.projection.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.0.norm.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.0.norm.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.norm1.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.norm1.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.attn.attn.in_proj_weight - torch.Size([192, 64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.attn.attn.in_proj_bias - torch.Size([192]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.attn.attn.out_proj.weight - torch.Size([64, 64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.attn.attn.out_proj.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.attn.sr.weight - torch.Size([64, 64, 8, 8]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.attn.sr.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.attn.norm.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.attn.norm.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.norm2.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.norm2.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.ffn.layers.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.ffn.layers.0.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.ffn.layers.1.weight - torch.Size([256, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.ffn.layers.1.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.ffn.layers.4.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.ffn.layers.4.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.norm1.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.norm1.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.attn.attn.in_proj_weight - torch.Size([192, 64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.attn.attn.in_proj_bias - torch.Size([192]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.attn.attn.out_proj.weight - torch.Size([64, 64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.attn.attn.out_proj.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.attn.sr.weight - torch.Size([64, 64, 8, 8]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.attn.sr.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.attn.norm.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.attn.norm.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.norm2.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.norm2.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.ffn.layers.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.ffn.layers.0.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.ffn.layers.1.weight - torch.Size([256, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.ffn.layers.1.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.ffn.layers.4.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.ffn.layers.4.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.norm1.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.norm1.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.attn.attn.in_proj_weight - torch.Size([192, 64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.attn.attn.in_proj_bias - torch.Size([192]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.attn.attn.out_proj.weight - torch.Size([64, 64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.attn.attn.out_proj.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.attn.sr.weight - torch.Size([64, 64, 8, 8]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.attn.sr.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.attn.norm.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.attn.norm.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.norm2.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.norm2.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.ffn.layers.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.ffn.layers.0.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.ffn.layers.1.weight - torch.Size([256, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.ffn.layers.1.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.ffn.layers.4.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.ffn.layers.4.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.2.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.2.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.0.projection.weight - torch.Size([128, 64, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.0.projection.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.0.norm.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.0.norm.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.norm1.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.norm1.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.attn.attn.in_proj_weight - torch.Size([384, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.attn.attn.in_proj_bias - torch.Size([384]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.attn.attn.out_proj.weight - torch.Size([128, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.attn.attn.out_proj.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.attn.sr.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.attn.norm.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.attn.norm.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.norm2.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.norm2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.ffn.layers.0.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.ffn.layers.1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.ffn.layers.4.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.norm1.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.norm1.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.attn.attn.in_proj_weight - torch.Size([384, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.attn.attn.in_proj_bias - torch.Size([384]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.attn.attn.out_proj.weight - torch.Size([128, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.attn.attn.out_proj.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.attn.sr.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.attn.norm.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.attn.norm.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.norm2.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.norm2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.ffn.layers.0.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.ffn.layers.1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.ffn.layers.4.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.norm1.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.norm1.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.attn.attn.in_proj_weight - torch.Size([384, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.attn.attn.in_proj_bias - torch.Size([384]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.attn.attn.out_proj.weight - torch.Size([128, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.attn.attn.out_proj.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.attn.sr.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.attn.norm.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.attn.norm.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.norm2.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.norm2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.ffn.layers.0.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.ffn.layers.1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.ffn.layers.4.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.norm1.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.norm1.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.attn.attn.in_proj_weight - torch.Size([384, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.attn.attn.in_proj_bias - torch.Size([384]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.attn.attn.out_proj.weight - torch.Size([128, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.attn.attn.out_proj.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.attn.sr.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.attn.norm.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.attn.norm.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.norm2.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.norm2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.ffn.layers.0.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.ffn.layers.1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.ffn.layers.4.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.norm1.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.norm1.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.attn.attn.in_proj_weight - torch.Size([384, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.attn.attn.in_proj_bias - torch.Size([384]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.attn.attn.out_proj.weight - torch.Size([128, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.attn.attn.out_proj.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.attn.sr.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.attn.norm.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.attn.norm.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.norm2.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.norm2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.ffn.layers.0.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.ffn.layers.1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.ffn.layers.4.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.norm1.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.norm1.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.attn.attn.in_proj_weight - torch.Size([384, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.attn.attn.in_proj_bias - torch.Size([384]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.attn.attn.out_proj.weight - torch.Size([128, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.attn.attn.out_proj.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.attn.sr.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.attn.norm.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.attn.norm.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.norm2.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.norm2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.ffn.layers.0.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.ffn.layers.1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.ffn.layers.4.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.2.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.0.projection.weight - torch.Size([320, 128, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.0.projection.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.0.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.0.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.0.projection.weight - torch.Size([512, 320, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.0.projection.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.0.norm.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.0.norm.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.attn.attn.in_proj_weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.attn.attn.in_proj_bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.attn.attn.out_proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.attn.attn.out_proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.ffn.layers.0.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.ffn.layers.0.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.ffn.layers.1.weight - torch.Size([2048, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.ffn.layers.1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.ffn.layers.4.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.ffn.layers.4.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.attn.attn.in_proj_weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.attn.attn.in_proj_bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.attn.attn.out_proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.attn.attn.out_proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.ffn.layers.0.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.ffn.layers.0.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.ffn.layers.1.weight - torch.Size([2048, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.ffn.layers.1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.ffn.layers.4.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.ffn.layers.4.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.attn.attn.in_proj_weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.attn.attn.in_proj_bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.attn.attn.out_proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.attn.attn.out_proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.ffn.layers.0.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.ffn.layers.0.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.ffn.layers.1.weight - torch.Size([2048, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.ffn.layers.1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.ffn.layers.4.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.ffn.layers.4.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

decode_head.decode_module.conv_seg.weight - torch.Size([59, 512, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.decode_module.conv_seg.bias - torch.Size([59]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.decode_module.convs.0.conv.weight - torch.Size([512, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.0.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.0.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.1.conv.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.1.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.1.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.2.conv.weight - torch.Size([512, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.2.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.2.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.3.conv.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.3.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.3.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.fusion_conv.conv.weight - torch.Size([512, 2048, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

decode_head.decode_module.fusion_conv.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.fusion_conv.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.clip.cls_token - torch.Size([1, 1, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.pos_embed - torch.Size([1, 197, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.patch_embed.projection.weight - torch.Size([768, 3, 16, 16]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.0.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.0.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.0.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.0.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.0.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.0.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.0.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.0.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.0.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.0.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.0.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.0.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.1.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.1.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.1.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.1.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.1.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.1.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.1.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.1.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.1.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.1.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.1.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.1.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.2.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.2.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.2.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.2.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.2.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.2.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.2.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.2.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.2.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.2.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.2.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.2.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.3.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.3.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.3.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.3.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.3.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.3.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.3.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.3.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.3.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.3.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.3.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.3.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.4.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.4.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.4.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.4.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.4.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.4.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.4.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.4.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.4.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.4.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.4.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.4.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.5.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.5.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.5.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.5.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.5.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.5.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.5.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.5.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.5.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.5.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.5.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.5.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.6.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.6.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.6.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.6.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.6.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.6.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.6.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.6.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.6.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.6.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.6.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.6.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.7.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.7.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.7.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.7.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.7.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.7.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.7.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.7.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.7.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.7.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.7.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.7.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.8.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.8.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.8.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.8.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.8.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.8.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.8.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.8.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.8.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.8.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.8.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.8.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.9.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.9.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.9.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.9.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.9.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.9.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.9.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.9.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.9.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.9.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.9.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.9.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.10.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.10.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.10.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.10.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.10.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.10.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.10.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.10.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.10.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.10.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.10.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.10.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.11.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.11.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.11.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.11.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.11.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.11.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.11.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.11.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.11.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.11.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.11.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layers.11.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.ln0.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.ln0.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.proj.weight - torch.Size([512, 768, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  
2023-02-19 12:48:24,300 - mmseg - INFO - EncoderDecoder(
  (backbone): MixVisionTransformer(
    (layers): ModuleList(
      (0): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
          (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0): TransformerEncoderLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): TransformerEncoderLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): TransformerEncoderLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      )
      (1): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (3): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (4): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (5): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
      (2): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (3): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (4): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (5): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (6): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (7): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (8): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (9): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (10): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (11): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (12): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (13): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (14): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (15): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (16): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (17): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (18): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (19): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (20): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (21): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (22): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (23): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (24): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (25): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (26): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (27): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (28): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (29): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (30): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (31): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (32): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (33): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (34): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (35): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (36): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (37): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (38): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (39): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      )
      (3): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0): TransformerEncoderLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): TransformerEncoderLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): TransformerEncoderLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  init_cfg={'type': 'Pretrained', 'checkpoint': 'pretrain/mit_b5_weight.pth'}
  (decode_head): MaskClipPlusSegformerHead(
    input_transform=None, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss()
    (decode_module): SegformerHead(
      input_transform=multiple_select, ignore_index=255, align_corners=False
      (loss_decode): CrossEntropyLoss()
      (conv_seg): Conv2d(512, 59, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout2d(p=0.1, inplace=False)
      (convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (1): ConvModule(
          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (2): ConvModule(
          (conv): Conv2d(320, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (3): ConvModule(
          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
      )
      (fusion_conv): ConvModule(
        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
    (clip): VisionTransformer(
      (patch_embed): PatchEmbed(
        (adap_padding): AdaptivePadding()
        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      )
      (drop_after_pos): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): DropPath()
          )
          (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU()
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=768, out_features=3072, bias=True)
                (1): GELU()
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=3072, out_features=768, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (1): TransformerEncoderLayer(
          (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): DropPath()
          )
          (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU()
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=768, out_features=3072, bias=True)
                (1): GELU()
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=3072, out_features=768, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (2): TransformerEncoderLayer(
          (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): DropPath()
          )
          (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU()
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=768, out_features=3072, bias=True)
                (1): GELU()
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=3072, out_features=768, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (3): TransformerEncoderLayer(
          (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): DropPath()
          )
          (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU()
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=768, out_features=3072, bias=True)
                (1): GELU()
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=3072, out_features=768, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (4): TransformerEncoderLayer(
          (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): DropPath()
          )
          (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU()
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=768, out_features=3072, bias=True)
                (1): GELU()
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=3072, out_features=768, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (5): TransformerEncoderLayer(
          (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): DropPath()
          )
          (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU()
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=768, out_features=3072, bias=True)
                (1): GELU()
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=3072, out_features=768, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (6): TransformerEncoderLayer(
          (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): DropPath()
          )
          (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU()
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=768, out_features=3072, bias=True)
                (1): GELU()
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=3072, out_features=768, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (7): TransformerEncoderLayer(
          (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): DropPath()
          )
          (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU()
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=768, out_features=3072, bias=True)
                (1): GELU()
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=3072, out_features=768, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (8): TransformerEncoderLayer(
          (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): DropPath()
          )
          (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU()
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=768, out_features=3072, bias=True)
                (1): GELU()
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=3072, out_features=768, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (9): TransformerEncoderLayer(
          (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): DropPath()
          )
          (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU()
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=768, out_features=3072, bias=True)
                (1): GELU()
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=3072, out_features=768, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (10): TransformerEncoderLayer(
          (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): DropPath()
          )
          (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU()
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=768, out_features=3072, bias=True)
                (1): GELU()
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=3072, out_features=768, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (11): TransformerEncoderLayer(
          (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): DropPath()
          )
          (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU()
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=768, out_features=3072, bias=True)
                (1): GELU()
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=3072, out_features=768, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
      )
      (ln0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
    (proj): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
)
2023-02-19 12:48:24,405 - mmseg - INFO - Loaded 4996 images
2023-02-19 12:48:26,669 - mmseg - INFO - Loaded 5104 images
2023-02-19 12:48:26,669 - mmseg - INFO - Start running, host: root@workspace-p4cxarvku5qr-0, work_dir: /root/sj/MaskCLIP_SegFormer/work_dirs/anno_free/vit-sfb5
2023-02-19 12:48:26,669 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-02-19 12:48:26,670 - mmseg - INFO - workflow: [('train', 1)], max: 8000 iters
2023-02-19 12:48:26,670 - mmseg - INFO - Checkpoints will be saved to /root/sj/MaskCLIP_SegFormer/work_dirs/anno_free/vit-sfb5 by HardDiskBackend.
2023-02-19 12:48:28,405 - mmseg - INFO - Start clip guided training
2023-02-19 12:49:38,621 - mmseg - INFO - Iter [50/8000]	lr: 7.792e-07, eta: 3:08:43, time: 1.424, data_time: 0.026, memory: 20583, decode.loss_ce: 2.8394, decode.acc_seg: 1.1851, loss: 2.8394
2023-02-19 12:50:30,547 - mmseg - INFO - Iter [100/8000]	lr: 1.564e-06, eta: 2:42:08, time: 1.039, data_time: 0.016, memory: 20583, decode.loss_ce: 2.9015, decode.acc_seg: 1.5977, loss: 2.9015
2023-02-19 12:51:22,561 - mmseg - INFO - Iter [150/8000]	lr: 2.340e-06, eta: 2:32:46, time: 1.040, data_time: 0.016, memory: 20583, decode.loss_ce: 2.8811, decode.acc_seg: 2.8432, loss: 2.8811
2023-02-19 12:52:14,582 - mmseg - INFO - Iter [200/8000]	lr: 3.105e-06, eta: 2:27:39, time: 1.040, data_time: 0.016, memory: 20583, decode.loss_ce: 2.7903, decode.acc_seg: 6.5366, loss: 2.7903
2023-02-19 12:53:06,661 - mmseg - INFO - Iter [250/8000]	lr: 3.860e-06, eta: 2:24:16, time: 1.042, data_time: 0.016, memory: 20583, decode.loss_ce: 2.7343, decode.acc_seg: 10.2417, loss: 2.7343
2023-02-19 12:53:58,708 - mmseg - INFO - Iter [300/8000]	lr: 4.605e-06, eta: 2:21:43, time: 1.041, data_time: 0.016, memory: 20583, decode.loss_ce: 2.5655, decode.acc_seg: 13.7390, loss: 2.5655
2023-02-19 12:54:50,750 - mmseg - INFO - Iter [350/8000]	lr: 5.340e-06, eta: 2:19:38, time: 1.041, data_time: 0.016, memory: 20583, decode.loss_ce: 2.4515, decode.acc_seg: 17.6291, loss: 2.4515
2023-02-19 12:55:42,885 - mmseg - INFO - Iter [400/8000]	lr: 6.066e-06, eta: 2:17:54, time: 1.043, data_time: 0.016, memory: 20583, decode.loss_ce: 2.3240, decode.acc_seg: 21.4320, loss: 2.3240
2023-02-19 12:56:35,041 - mmseg - INFO - Iter [450/8000]	lr: 6.781e-06, eta: 2:16:21, time: 1.043, data_time: 0.016, memory: 20583, decode.loss_ce: 2.1669, decode.acc_seg: 24.0016, loss: 2.1669
2023-02-19 12:57:27,179 - mmseg - INFO - Iter [500/8000]	lr: 7.486e-06, eta: 2:14:56, time: 1.043, data_time: 0.017, memory: 20583, decode.loss_ce: 2.0125, decode.acc_seg: 25.9202, loss: 2.0125
2023-02-19 12:58:19,178 - mmseg - INFO - Iter [550/8000]	lr: 8.181e-06, eta: 2:13:35, time: 1.040, data_time: 0.016, memory: 20583, decode.loss_ce: 1.9452, decode.acc_seg: 26.9022, loss: 1.9452
2023-02-19 12:59:11,200 - mmseg - INFO - Iter [600/8000]	lr: 8.866e-06, eta: 2:12:20, time: 1.040, data_time: 0.016, memory: 20583, decode.loss_ce: 1.9258, decode.acc_seg: 30.3841, loss: 1.9258
2023-02-19 13:00:05,657 - mmseg - INFO - Iter [650/8000]	lr: 9.542e-06, eta: 2:11:35, time: 1.089, data_time: 0.063, memory: 20583, decode.loss_ce: 1.9294, decode.acc_seg: 29.9169, loss: 1.9294
2023-02-19 13:00:57,784 - mmseg - INFO - Iter [700/8000]	lr: 1.021e-05, eta: 2:10:25, time: 1.043, data_time: 0.016, memory: 20583, decode.loss_ce: 1.8578, decode.acc_seg: 30.9234, loss: 1.8578
2023-02-19 13:01:49,943 - mmseg - INFO - Iter [750/8000]	lr: 1.086e-05, eta: 2:09:17, time: 1.043, data_time: 0.016, memory: 20583, decode.loss_ce: 1.8038, decode.acc_seg: 31.3731, loss: 1.8038
2023-02-19 13:02:42,062 - mmseg - INFO - Iter [800/8000]	lr: 1.151e-05, eta: 2:08:11, time: 1.042, data_time: 0.016, memory: 20583, decode.loss_ce: 1.7919, decode.acc_seg: 31.5311, loss: 1.7919
2023-02-19 13:03:34,120 - mmseg - INFO - Iter [850/8000]	lr: 1.214e-05, eta: 2:07:06, time: 1.041, data_time: 0.015, memory: 20583, decode.loss_ce: 1.7677, decode.acc_seg: 32.3285, loss: 1.7677
2023-02-19 13:04:26,180 - mmseg - INFO - Iter [900/8000]	lr: 1.277e-05, eta: 2:06:03, time: 1.041, data_time: 0.016, memory: 20583, decode.loss_ce: 1.7502, decode.acc_seg: 32.6037, loss: 1.7502
2023-02-19 13:05:18,282 - mmseg - INFO - Iter [950/8000]	lr: 1.338e-05, eta: 2:05:01, time: 1.042, data_time: 0.016, memory: 20583, decode.loss_ce: 1.6444, decode.acc_seg: 32.6260, loss: 1.6444
2023-02-19 13:06:10,192 - mmseg - INFO - Exp name: maskclip_plus_vit16_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 13:06:10,192 - mmseg - INFO - Iter [1000/8000]	lr: 1.399e-05, eta: 2:03:59, time: 1.038, data_time: 0.015, memory: 20583, decode.loss_ce: 1.5794, decode.acc_seg: 32.9031, loss: 1.5794
2023-02-19 13:07:01,992 - mmseg - INFO - Iter [1050/8000]	lr: 1.458e-05, eta: 2:02:57, time: 1.036, data_time: 0.015, memory: 20583, decode.loss_ce: 1.6491, decode.acc_seg: 35.0888, loss: 1.6491
2023-02-19 13:07:53,880 - mmseg - INFO - Iter [1100/8000]	lr: 1.517e-05, eta: 2:01:56, time: 1.038, data_time: 0.015, memory: 20583, decode.loss_ce: 1.6580, decode.acc_seg: 34.3918, loss: 1.6580
2023-02-19 13:08:45,784 - mmseg - INFO - Iter [1150/8000]	lr: 1.574e-05, eta: 2:00:57, time: 1.038, data_time: 0.016, memory: 20583, decode.loss_ce: 1.5818, decode.acc_seg: 34.8592, loss: 1.5818
2023-02-19 13:09:37,679 - mmseg - INFO - Iter [1200/8000]	lr: 1.631e-05, eta: 1:59:58, time: 1.038, data_time: 0.016, memory: 20583, decode.loss_ce: 1.5715, decode.acc_seg: 34.0456, loss: 1.5715
2023-02-19 13:10:31,887 - mmseg - INFO - Iter [1250/8000]	lr: 1.686e-05, eta: 1:59:12, time: 1.084, data_time: 0.062, memory: 20583, decode.loss_ce: 1.6650, decode.acc_seg: 34.3161, loss: 1.6650
2023-02-19 13:11:23,663 - mmseg - INFO - Iter [1300/8000]	lr: 1.741e-05, eta: 1:58:12, time: 1.035, data_time: 0.015, memory: 20583, decode.loss_ce: 1.6539, decode.acc_seg: 35.0204, loss: 1.6539
2023-02-19 13:12:15,592 - mmseg - INFO - Iter [1350/8000]	lr: 1.794e-05, eta: 1:57:15, time: 1.039, data_time: 0.016, memory: 20583, decode.loss_ce: 1.5880, decode.acc_seg: 33.5008, loss: 1.5880
2023-02-19 13:13:07,555 - mmseg - INFO - Iter [1400/8000]	lr: 1.847e-05, eta: 1:56:17, time: 1.039, data_time: 0.016, memory: 20583, decode.loss_ce: 1.5616, decode.acc_seg: 34.4323, loss: 1.5616
2023-02-19 13:13:59,440 - mmseg - INFO - Iter [1450/8000]	lr: 1.898e-05, eta: 1:55:20, time: 1.038, data_time: 0.016, memory: 20583, decode.loss_ce: 1.5219, decode.acc_seg: 33.0578, loss: 1.5219
2023-02-19 13:14:51,391 - mmseg - INFO - Iter [1500/8000]	lr: 1.949e-05, eta: 1:54:23, time: 1.039, data_time: 0.016, memory: 20583, decode.loss_ce: 1.5165, decode.acc_seg: 35.6888, loss: 1.5165
2023-02-19 13:15:43,337 - mmseg - INFO - Iter [1550/8000]	lr: 1.935e-05, eta: 1:53:27, time: 1.039, data_time: 0.016, memory: 20583, decode.loss_ce: 1.6071, decode.acc_seg: 34.7072, loss: 1.6071
2023-02-19 13:16:35,259 - mmseg - INFO - Iter [1600/8000]	lr: 1.920e-05, eta: 1:52:31, time: 1.038, data_time: 0.016, memory: 20583, decode.loss_ce: 1.5558, decode.acc_seg: 34.6788, loss: 1.5558
2023-02-19 13:17:27,155 - mmseg - INFO - Iter [1650/8000]	lr: 1.905e-05, eta: 1:51:35, time: 1.038, data_time: 0.016, memory: 20583, decode.loss_ce: 1.5412, decode.acc_seg: 37.5577, loss: 1.5412
2023-02-19 13:18:19,044 - mmseg - INFO - Iter [1700/8000]	lr: 1.890e-05, eta: 1:50:39, time: 1.038, data_time: 0.015, memory: 20583, decode.loss_ce: 1.5184, decode.acc_seg: 35.7711, loss: 1.5184
2023-02-19 13:19:10,998 - mmseg - INFO - Iter [1750/8000]	lr: 1.875e-05, eta: 1:49:44, time: 1.039, data_time: 0.015, memory: 20583, decode.loss_ce: 1.4823, decode.acc_seg: 36.8700, loss: 1.4823
2023-02-19 13:20:03,121 - mmseg - INFO - Iter [1800/8000]	lr: 1.860e-05, eta: 1:48:49, time: 1.042, data_time: 0.016, memory: 20583, decode.loss_ce: 1.5861, decode.acc_seg: 36.1793, loss: 1.5861
2023-02-19 13:20:55,106 - mmseg - INFO - Iter [1850/8000]	lr: 1.845e-05, eta: 1:47:54, time: 1.040, data_time: 0.015, memory: 20583, decode.loss_ce: 1.5156, decode.acc_seg: 36.3171, loss: 1.5156
2023-02-19 13:21:49,414 - mmseg - INFO - Iter [1900/8000]	lr: 1.830e-05, eta: 1:47:07, time: 1.086, data_time: 0.062, memory: 20583, decode.loss_ce: 1.4942, decode.acc_seg: 36.4362, loss: 1.4942
2023-02-19 13:22:41,376 - mmseg - INFO - Iter [1950/8000]	lr: 1.815e-05, eta: 1:46:12, time: 1.039, data_time: 0.015, memory: 20583, decode.loss_ce: 1.5473, decode.acc_seg: 36.2793, loss: 1.5473
2023-02-19 13:23:33,403 - mmseg - INFO - Exp name: maskclip_plus_vit16_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 13:23:33,403 - mmseg - INFO - Iter [2000/8000]	lr: 1.800e-05, eta: 1:45:17, time: 1.041, data_time: 0.016, memory: 20583, decode.loss_ce: 1.5558, decode.acc_seg: 37.4653, loss: 1.5558
2023-02-19 13:24:25,486 - mmseg - INFO - Iter [2050/8000]	lr: 1.785e-05, eta: 1:44:23, time: 1.042, data_time: 0.016, memory: 20583, decode.loss_ce: 1.4118, decode.acc_seg: 35.1186, loss: 1.4118
2023-02-19 13:25:17,655 - mmseg - INFO - Iter [2100/8000]	lr: 1.770e-05, eta: 1:43:29, time: 1.043, data_time: 0.016, memory: 20583, decode.loss_ce: 1.4939, decode.acc_seg: 37.3100, loss: 1.4939
2023-02-19 13:26:09,832 - mmseg - INFO - Iter [2150/8000]	lr: 1.755e-05, eta: 1:42:35, time: 1.044, data_time: 0.016, memory: 20583, decode.loss_ce: 1.4799, decode.acc_seg: 36.2667, loss: 1.4799
2023-02-19 13:27:01,760 - mmseg - INFO - Iter [2200/8000]	lr: 1.740e-05, eta: 1:41:41, time: 1.039, data_time: 0.016, memory: 20583, decode.loss_ce: 1.4614, decode.acc_seg: 36.3704, loss: 1.4614
2023-02-19 13:27:53,868 - mmseg - INFO - Iter [2250/8000]	lr: 1.725e-05, eta: 1:40:47, time: 1.042, data_time: 0.017, memory: 20583, decode.loss_ce: 1.3940, decode.acc_seg: 35.5886, loss: 1.3940
2023-02-19 13:28:45,961 - mmseg - INFO - Iter [2300/8000]	lr: 1.710e-05, eta: 1:39:53, time: 1.042, data_time: 0.017, memory: 20583, decode.loss_ce: 1.4588, decode.acc_seg: 37.5589, loss: 1.4588
2023-02-19 13:29:38,013 - mmseg - INFO - Iter [2350/8000]	lr: 1.695e-05, eta: 1:38:59, time: 1.041, data_time: 0.016, memory: 20583, decode.loss_ce: 1.5013, decode.acc_seg: 36.1953, loss: 1.5013
2023-02-19 13:30:30,034 - mmseg - INFO - Iter [2400/8000]	lr: 1.680e-05, eta: 1:38:06, time: 1.040, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3832, decode.acc_seg: 35.4697, loss: 1.3832
2023-02-19 13:31:22,024 - mmseg - INFO - Iter [2450/8000]	lr: 1.665e-05, eta: 1:37:12, time: 1.040, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3929, decode.acc_seg: 37.2874, loss: 1.3929
2023-02-19 13:32:16,218 - mmseg - INFO - Iter [2500/8000]	lr: 1.650e-05, eta: 1:36:23, time: 1.084, data_time: 0.062, memory: 20583, decode.loss_ce: 1.4362, decode.acc_seg: 34.9647, loss: 1.4362
2023-02-19 13:33:08,013 - mmseg - INFO - Iter [2550/8000]	lr: 1.635e-05, eta: 1:35:29, time: 1.036, data_time: 0.016, memory: 20583, decode.loss_ce: 1.4049, decode.acc_seg: 36.8503, loss: 1.4049
2023-02-19 13:33:59,803 - mmseg - INFO - Iter [2600/8000]	lr: 1.620e-05, eta: 1:34:34, time: 1.036, data_time: 0.016, memory: 20583, decode.loss_ce: 1.4467, decode.acc_seg: 36.9198, loss: 1.4467
2023-02-19 13:34:51,676 - mmseg - INFO - Iter [2650/8000]	lr: 1.605e-05, eta: 1:33:41, time: 1.037, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3825, decode.acc_seg: 37.2740, loss: 1.3825
2023-02-19 13:35:43,644 - mmseg - INFO - Iter [2700/8000]	lr: 1.590e-05, eta: 1:32:47, time: 1.039, data_time: 0.017, memory: 20583, decode.loss_ce: 1.4086, decode.acc_seg: 36.8338, loss: 1.4086
2023-02-19 13:36:35,648 - mmseg - INFO - Iter [2750/8000]	lr: 1.575e-05, eta: 1:31:53, time: 1.040, data_time: 0.017, memory: 20583, decode.loss_ce: 1.4000, decode.acc_seg: 35.5696, loss: 1.4000
2023-02-19 13:37:28,000 - mmseg - INFO - Iter [2800/8000]	lr: 1.560e-05, eta: 1:31:01, time: 1.047, data_time: 0.017, memory: 20583, decode.loss_ce: 1.4672, decode.acc_seg: 37.5318, loss: 1.4672
2023-02-19 13:38:20,218 - mmseg - INFO - Iter [2850/8000]	lr: 1.545e-05, eta: 1:30:08, time: 1.044, data_time: 0.017, memory: 20583, decode.loss_ce: 1.4012, decode.acc_seg: 36.1687, loss: 1.4012
2023-02-19 13:39:12,371 - mmseg - INFO - Iter [2900/8000]	lr: 1.530e-05, eta: 1:29:14, time: 1.043, data_time: 0.017, memory: 20583, decode.loss_ce: 1.4242, decode.acc_seg: 37.8501, loss: 1.4242
2023-02-19 13:40:04,520 - mmseg - INFO - Iter [2950/8000]	lr: 1.515e-05, eta: 1:28:21, time: 1.043, data_time: 0.016, memory: 20583, decode.loss_ce: 1.4562, decode.acc_seg: 37.0403, loss: 1.4562
2023-02-19 13:40:56,654 - mmseg - INFO - Exp name: maskclip_plus_vit16_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 13:40:56,655 - mmseg - INFO - Iter [3000/8000]	lr: 1.500e-05, eta: 1:27:28, time: 1.043, data_time: 0.016, memory: 20583, decode.loss_ce: 1.4119, decode.acc_seg: 36.0578, loss: 1.4119
2023-02-19 13:41:48,784 - mmseg - INFO - Iter [3050/8000]	lr: 1.485e-05, eta: 1:26:35, time: 1.043, data_time: 0.016, memory: 20583, decode.loss_ce: 1.4015, decode.acc_seg: 36.8861, loss: 1.4015
2023-02-19 13:42:40,804 - mmseg - INFO - Iter [3100/8000]	lr: 1.470e-05, eta: 1:25:42, time: 1.040, data_time: 0.016, memory: 20583, decode.loss_ce: 1.4377, decode.acc_seg: 37.2252, loss: 1.4377
2023-02-19 13:43:35,064 - mmseg - INFO - Iter [3150/8000]	lr: 1.455e-05, eta: 1:24:52, time: 1.085, data_time: 0.063, memory: 20583, decode.loss_ce: 1.3349, decode.acc_seg: 35.3268, loss: 1.3349
2023-02-19 13:44:27,075 - mmseg - INFO - Iter [3200/8000]	lr: 1.440e-05, eta: 1:23:59, time: 1.040, data_time: 0.017, memory: 20583, decode.loss_ce: 1.4306, decode.acc_seg: 37.2077, loss: 1.4306
2023-02-19 13:45:19,071 - mmseg - INFO - Iter [3250/8000]	lr: 1.425e-05, eta: 1:23:06, time: 1.040, data_time: 0.017, memory: 20583, decode.loss_ce: 1.4089, decode.acc_seg: 38.2901, loss: 1.4089
2023-02-19 13:46:11,121 - mmseg - INFO - Iter [3300/8000]	lr: 1.410e-05, eta: 1:22:13, time: 1.041, data_time: 0.017, memory: 20583, decode.loss_ce: 1.3307, decode.acc_seg: 34.0284, loss: 1.3307
2023-02-19 13:47:03,329 - mmseg - INFO - Iter [3350/8000]	lr: 1.395e-05, eta: 1:21:20, time: 1.044, data_time: 0.017, memory: 20583, decode.loss_ce: 1.3853, decode.acc_seg: 37.8136, loss: 1.3853
2023-02-19 13:47:55,484 - mmseg - INFO - Iter [3400/8000]	lr: 1.380e-05, eta: 1:20:27, time: 1.043, data_time: 0.017, memory: 20583, decode.loss_ce: 1.3920, decode.acc_seg: 38.1355, loss: 1.3920
2023-02-19 13:48:47,593 - mmseg - INFO - Iter [3450/8000]	lr: 1.365e-05, eta: 1:19:34, time: 1.042, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3510, decode.acc_seg: 35.7351, loss: 1.3510
2023-02-19 13:49:39,715 - mmseg - INFO - Iter [3500/8000]	lr: 1.350e-05, eta: 1:18:41, time: 1.042, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3269, decode.acc_seg: 35.8879, loss: 1.3269
2023-02-19 13:50:31,780 - mmseg - INFO - Iter [3550/8000]	lr: 1.335e-05, eta: 1:17:48, time: 1.041, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3816, decode.acc_seg: 37.1670, loss: 1.3816
2023-02-19 13:51:23,730 - mmseg - INFO - Iter [3600/8000]	lr: 1.320e-05, eta: 1:16:55, time: 1.039, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3661, decode.acc_seg: 37.8831, loss: 1.3661
2023-02-19 13:52:15,712 - mmseg - INFO - Iter [3650/8000]	lr: 1.305e-05, eta: 1:16:02, time: 1.040, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3342, decode.acc_seg: 37.2531, loss: 1.3342
2023-02-19 13:53:07,656 - mmseg - INFO - Iter [3700/8000]	lr: 1.290e-05, eta: 1:15:09, time: 1.039, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3881, decode.acc_seg: 36.8342, loss: 1.3881
2023-02-19 13:54:01,957 - mmseg - INFO - Iter [3750/8000]	lr: 1.275e-05, eta: 1:14:19, time: 1.086, data_time: 0.063, memory: 20583, decode.loss_ce: 1.3028, decode.acc_seg: 37.2045, loss: 1.3028
2023-02-19 13:54:53,801 - mmseg - INFO - Iter [3800/8000]	lr: 1.260e-05, eta: 1:13:25, time: 1.037, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3484, decode.acc_seg: 38.8753, loss: 1.3484
2023-02-19 13:55:45,718 - mmseg - INFO - Iter [3850/8000]	lr: 1.245e-05, eta: 1:12:32, time: 1.038, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3484, decode.acc_seg: 35.4972, loss: 1.3484
2023-02-19 13:56:37,785 - mmseg - INFO - Iter [3900/8000]	lr: 1.230e-05, eta: 1:11:40, time: 1.041, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3607, decode.acc_seg: 37.9904, loss: 1.3607
2023-02-19 13:57:29,835 - mmseg - INFO - Iter [3950/8000]	lr: 1.215e-05, eta: 1:10:47, time: 1.041, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3535, decode.acc_seg: 37.1525, loss: 1.3535
2023-02-19 13:58:21,918 - mmseg - INFO - Saving checkpoint at 4000 iterations
2023-02-19 13:58:24,065 - mmseg - INFO - Exp name: maskclip_plus_vit16_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 13:58:24,065 - mmseg - INFO - Iter [4000/8000]	lr: 1.200e-05, eta: 1:09:56, time: 1.085, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3992, decode.acc_seg: 37.9372, loss: 1.3992
2023-02-19 14:20:35,525 - mmseg - INFO - per class results:
2023-02-19 14:20:35,529 - mmseg - INFO - 
+-------------+-------+-------+-------+
|    Class    |  IoU  |  Acc  |  Prec |
+-------------+-------+-------+-------+
|  aeroplane  | 41.39 | 97.46 | 41.84 |
|     bag     | 14.68 | 22.69 | 29.37 |
|     bed     | 11.06 | 52.67 | 12.28 |
|  bedclothes | 11.29 | 13.44 | 41.36 |
|    bench    |  0.09 |  0.09 | 45.97 |
|   bicycle   | 55.53 | 86.85 | 60.63 |
|     bird    | 66.79 | 94.23 | 69.64 |
|     boat    | 35.79 | 86.06 | 37.99 |
|     book    | 29.31 | 31.77 | 79.12 |
|    bottle   |  56.5 | 75.19 | 69.45 |
|   building  | 15.44 | 15.91 | 83.89 |
|     bus     | 76.01 | 85.53 | 87.22 |
|   cabinet   | 10.29 | 30.69 | 13.41 |
|     car     | 69.31 | 88.63 | 76.07 |
|     cat     | 83.27 | 93.82 | 88.11 |
|   ceiling   |  33.4 | 73.48 | 37.97 |
|    chair    | 29.63 | 40.66 | 52.19 |
|    cloth    |  6.81 | 33.46 |  7.88 |
|   computer  |  7.72 | 28.99 |  9.53 |
|     cow     | 70.72 | 91.64 |  75.6 |
|     cup     |  6.27 | 20.83 |  8.24 |
|   curtain   | 33.22 | 41.87 | 61.66 |
|     dog     | 78.87 | 90.46 | 86.03 |
|     door    |  9.46 | 55.92 | 10.23 |
|    fence    | 20.74 | 23.82 | 61.56 |
|    floor    | 55.11 | 75.03 | 67.48 |
|    flower   | 17.12 | 43.22 | 22.08 |
|     food    |  8.14 | 89.29 |  8.22 |
|    grass    | 62.27 | 75.56 | 77.97 |
|    ground   |  22.7 | 34.97 | 39.27 |
|    horse    | 63.58 | 91.86 | 67.38 |
|   keyboard  | 68.68 | 75.88 | 87.85 |
|    light    | 13.18 | 67.98 | 14.06 |
|  motorbike  | 61.14 | 78.96 | 73.04 |
|   mountain  | 33.63 | 49.79 |  50.9 |
|    mouse    | 13.65 | 33.13 | 18.85 |
|    person   | 45.84 | 51.15 | 81.51 |
|    plate    |  3.33 |  7.01 |  5.97 |
|   platform  |  0.0  |  0.0  |  0.0  |
| pottedplant |  36.1 |  80.5 | 39.56 |
|     road    | 42.04 | 58.83 | 59.57 |
|     rock    | 29.36 | 70.24 | 33.53 |
|    sheep    | 72.41 | 90.13 | 78.65 |
|   shelves   |  6.25 |  6.59 | 54.26 |
|   sidewalk  | 10.29 | 69.89 | 10.76 |
|     sign    | 30.57 | 58.15 | 39.18 |
|     sky     | 61.31 | 61.77 | 98.82 |
|     snow    | 52.58 | 77.21 | 62.23 |
|     sofa    | 39.17 | 48.38 | 67.29 |
|    table    | 16.63 | 51.96 | 19.66 |
|    track    |  0.0  |  0.0  |  0.0  |
|    train    | 47.42 | 87.05 | 51.02 |
|     tree    | 47.16 | 51.35 | 85.25 |
|    truck    | 12.75 | 47.34 | 14.86 |
|  tvmonitor  | 39.39 | 46.53 | 71.96 |
|     wall    | 34.24 | 43.11 | 62.48 |
|    water    | 70.09 | 78.12 | 87.22 |
|    window   | 24.68 | 32.58 | 50.43 |
|     wood    |  7.77 | 41.13 |  8.74 |
+-------------+-------+-------+-------+
2023-02-19 14:20:35,529 - mmseg - INFO - Summary:
2023-02-19 14:20:35,529 - mmseg - INFO - 
+-------+-------+------+-------+
|  aAcc |  mIoU | mAcc | mPrec |
+-------+-------+------+-------+
| 57.99 | 34.27 | 55.1 | 48.43 |
+-------+-------+------+-------+
2023-02-19 14:20:35,541 - mmseg - INFO - Exp name: maskclip_plus_vit16_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 14:20:35,541 - mmseg - INFO - Iter(val) [5104]	aAcc: 0.5799, mIoU: 0.3427, mAcc: 0.5510, mPrec: 0.4843, IoU.aeroplane: 0.4139, IoU.bag: 0.1468, IoU.bed: 0.1106, IoU.bedclothes: 0.1129, IoU.bench: 0.0009, IoU.bicycle: 0.5553, IoU.bird: 0.6679, IoU.boat: 0.3579, IoU.book: 0.2931, IoU.bottle: 0.5650, IoU.building: 0.1544, IoU.bus: 0.7601, IoU.cabinet: 0.1029, IoU.car: 0.6931, IoU.cat: 0.8327, IoU.ceiling: 0.3340, IoU.chair: 0.2963, IoU.cloth: 0.0681, IoU.computer: 0.0772, IoU.cow: 0.7072, IoU.cup: 0.0627, IoU.curtain: 0.3322, IoU.dog: 0.7887, IoU.door: 0.0946, IoU.fence: 0.2074, IoU.floor: 0.5511, IoU.flower: 0.1712, IoU.food: 0.0814, IoU.grass: 0.6227, IoU.ground: 0.2270, IoU.horse: 0.6358, IoU.keyboard: 0.6868, IoU.light: 0.1318, IoU.motorbike: 0.6114, IoU.mountain: 0.3363, IoU.mouse: 0.1365, IoU.person: 0.4584, IoU.plate: 0.0333, IoU.platform: 0.0000, IoU.pottedplant: 0.3610, IoU.road: 0.4204, IoU.rock: 0.2936, IoU.sheep: 0.7241, IoU.shelves: 0.0625, IoU.sidewalk: 0.1029, IoU.sign: 0.3057, IoU.sky: 0.6131, IoU.snow: 0.5258, IoU.sofa: 0.3917, IoU.table: 0.1663, IoU.track: 0.0000, IoU.train: 0.4742, IoU.tree: 0.4716, IoU.truck: 0.1275, IoU.tvmonitor: 0.3939, IoU.wall: 0.3424, IoU.water: 0.7009, IoU.window: 0.2468, IoU.wood: 0.0777, Acc.aeroplane: 0.9746, Acc.bag: 0.2269, Acc.bed: 0.5267, Acc.bedclothes: 0.1344, Acc.bench: 0.0009, Acc.bicycle: 0.8685, Acc.bird: 0.9423, Acc.boat: 0.8606, Acc.book: 0.3177, Acc.bottle: 0.7519, Acc.building: 0.1591, Acc.bus: 0.8553, Acc.cabinet: 0.3069, Acc.car: 0.8863, Acc.cat: 0.9382, Acc.ceiling: 0.7348, Acc.chair: 0.4066, Acc.cloth: 0.3346, Acc.computer: 0.2899, Acc.cow: 0.9164, Acc.cup: 0.2083, Acc.curtain: 0.4187, Acc.dog: 0.9046, Acc.door: 0.5592, Acc.fence: 0.2382, Acc.floor: 0.7503, Acc.flower: 0.4322, Acc.food: 0.8929, Acc.grass: 0.7556, Acc.ground: 0.3497, Acc.horse: 0.9186, Acc.keyboard: 0.7588, Acc.light: 0.6798, Acc.motorbike: 0.7896, Acc.mountain: 0.4979, Acc.mouse: 0.3313, Acc.person: 0.5115, Acc.plate: 0.0701, Acc.platform: 0.0000, Acc.pottedplant: 0.8050, Acc.road: 0.5883, Acc.rock: 0.7024, Acc.sheep: 0.9013, Acc.shelves: 0.0659, Acc.sidewalk: 0.6989, Acc.sign: 0.5815, Acc.sky: 0.6177, Acc.snow: 0.7721, Acc.sofa: 0.4838, Acc.table: 0.5196, Acc.track: 0.0000, Acc.train: 0.8705, Acc.tree: 0.5135, Acc.truck: 0.4734, Acc.tvmonitor: 0.4653, Acc.wall: 0.4311, Acc.water: 0.7812, Acc.window: 0.3258, Acc.wood: 0.4113, Prec.aeroplane: 0.4184, Prec.bag: 0.2937, Prec.bed: 0.1228, Prec.bedclothes: 0.4136, Prec.bench: 0.4597, Prec.bicycle: 0.6063, Prec.bird: 0.6964, Prec.boat: 0.3799, Prec.book: 0.7912, Prec.bottle: 0.6945, Prec.building: 0.8389, Prec.bus: 0.8722, Prec.cabinet: 0.1341, Prec.car: 0.7607, Prec.cat: 0.8811, Prec.ceiling: 0.3797, Prec.chair: 0.5219, Prec.cloth: 0.0788, Prec.computer: 0.0953, Prec.cow: 0.7560, Prec.cup: 0.0824, Prec.curtain: 0.6166, Prec.dog: 0.8603, Prec.door: 0.1023, Prec.fence: 0.6156, Prec.floor: 0.6748, Prec.flower: 0.2208, Prec.food: 0.0822, Prec.grass: 0.7797, Prec.ground: 0.3927, Prec.horse: 0.6738, Prec.keyboard: 0.8785, Prec.light: 0.1406, Prec.motorbike: 0.7304, Prec.mountain: 0.5090, Prec.mouse: 0.1885, Prec.person: 0.8151, Prec.plate: 0.0597, Prec.platform: 0.0000, Prec.pottedplant: 0.3956, Prec.road: 0.5957, Prec.rock: 0.3353, Prec.sheep: 0.7865, Prec.shelves: 0.5426, Prec.sidewalk: 0.1076, Prec.sign: 0.3918, Prec.sky: 0.9882, Prec.snow: 0.6223, Prec.sofa: 0.6729, Prec.table: 0.1966, Prec.track: 0.0000, Prec.train: 0.5102, Prec.tree: 0.8525, Prec.truck: 0.1486, Prec.tvmonitor: 0.7196, Prec.wall: 0.6248, Prec.water: 0.8722, Prec.window: 0.5043, Prec.wood: 0.0874
2023-02-19 14:21:28,246 - mmseg - INFO - Iter [4050/8000]	lr: 1.185e-05, eta: 1:30:43, time: 27.684, data_time: 26.646, memory: 20583, decode.loss_ce: 1.3748, decode.acc_seg: 38.5325, loss: 1.3748
2023-02-19 14:22:21,026 - mmseg - INFO - Iter [4100/8000]	lr: 1.170e-05, eta: 1:29:18, time: 1.056, data_time: 0.017, memory: 20583, decode.loss_ce: 1.3458, decode.acc_seg: 38.2214, loss: 1.3458
2023-02-19 14:23:13,831 - mmseg - INFO - Iter [4150/8000]	lr: 1.155e-05, eta: 1:27:55, time: 1.056, data_time: 0.017, memory: 20583, decode.loss_ce: 1.3305, decode.acc_seg: 36.9340, loss: 1.3305
2023-02-19 14:24:06,613 - mmseg - INFO - Iter [4200/8000]	lr: 1.140e-05, eta: 1:26:32, time: 1.056, data_time: 0.017, memory: 20583, decode.loss_ce: 1.2909, decode.acc_seg: 34.9963, loss: 1.2909
2023-02-19 14:24:59,318 - mmseg - INFO - Iter [4250/8000]	lr: 1.125e-05, eta: 1:25:10, time: 1.054, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3265, decode.acc_seg: 37.1289, loss: 1.3265
2023-02-19 14:25:51,997 - mmseg - INFO - Iter [4300/8000]	lr: 1.110e-05, eta: 1:23:49, time: 1.054, data_time: 0.017, memory: 20583, decode.loss_ce: 1.3667, decode.acc_seg: 36.1526, loss: 1.3667
2023-02-19 14:26:44,595 - mmseg - INFO - Iter [4350/8000]	lr: 1.095e-05, eta: 1:22:28, time: 1.052, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3175, decode.acc_seg: 38.3925, loss: 1.3175
2023-02-19 14:27:39,306 - mmseg - INFO - Iter [4400/8000]	lr: 1.080e-05, eta: 1:21:09, time: 1.094, data_time: 0.062, memory: 20583, decode.loss_ce: 1.3180, decode.acc_seg: 38.7672, loss: 1.3180
2023-02-19 14:28:31,558 - mmseg - INFO - Iter [4450/8000]	lr: 1.065e-05, eta: 1:19:49, time: 1.045, data_time: 0.015, memory: 20583, decode.loss_ce: 1.3228, decode.acc_seg: 37.6515, loss: 1.3228
2023-02-19 14:29:23,937 - mmseg - INFO - Iter [4500/8000]	lr: 1.050e-05, eta: 1:18:30, time: 1.048, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3539, decode.acc_seg: 38.8120, loss: 1.3539
2023-02-19 14:30:16,310 - mmseg - INFO - Iter [4550/8000]	lr: 1.035e-05, eta: 1:17:11, time: 1.047, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3329, decode.acc_seg: 37.5705, loss: 1.3329
2023-02-19 14:31:08,708 - mmseg - INFO - Iter [4600/8000]	lr: 1.020e-05, eta: 1:15:53, time: 1.048, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3202, decode.acc_seg: 37.6511, loss: 1.3202
2023-02-19 14:32:01,173 - mmseg - INFO - Iter [4650/8000]	lr: 1.005e-05, eta: 1:14:36, time: 1.049, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3245, decode.acc_seg: 38.7468, loss: 1.3245
2023-02-19 14:32:53,637 - mmseg - INFO - Iter [4700/8000]	lr: 9.903e-06, eta: 1:13:19, time: 1.049, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3435, decode.acc_seg: 38.3448, loss: 1.3435
2023-02-19 14:33:46,153 - mmseg - INFO - Iter [4750/8000]	lr: 9.753e-06, eta: 1:12:03, time: 1.050, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3276, decode.acc_seg: 36.8765, loss: 1.3276
2023-02-19 14:34:38,768 - mmseg - INFO - Iter [4800/8000]	lr: 9.603e-06, eta: 1:10:47, time: 1.052, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2635, decode.acc_seg: 37.6453, loss: 1.2635
2023-02-19 14:35:31,164 - mmseg - INFO - Iter [4850/8000]	lr: 9.453e-06, eta: 1:09:32, time: 1.048, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3543, decode.acc_seg: 38.2963, loss: 1.3543
2023-02-19 14:36:23,686 - mmseg - INFO - Iter [4900/8000]	lr: 9.303e-06, eta: 1:08:17, time: 1.050, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3015, decode.acc_seg: 38.3225, loss: 1.3015
2023-02-19 14:37:16,277 - mmseg - INFO - Iter [4950/8000]	lr: 9.153e-06, eta: 1:07:02, time: 1.052, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3865, decode.acc_seg: 38.2979, loss: 1.3865
2023-02-19 14:38:11,097 - mmseg - INFO - Exp name: maskclip_plus_vit16_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 14:38:11,097 - mmseg - INFO - Iter [5000/8000]	lr: 9.003e-06, eta: 1:05:50, time: 1.096, data_time: 0.063, memory: 20583, decode.loss_ce: 1.3028, decode.acc_seg: 38.4645, loss: 1.3028
2023-02-19 14:39:03,489 - mmseg - INFO - Iter [5050/8000]	lr: 8.853e-06, eta: 1:04:36, time: 1.048, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3220, decode.acc_seg: 36.0989, loss: 1.3220
2023-02-19 14:39:56,013 - mmseg - INFO - Iter [5100/8000]	lr: 8.703e-06, eta: 1:03:23, time: 1.050, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2951, decode.acc_seg: 37.8296, loss: 1.2951
2023-02-19 14:40:48,640 - mmseg - INFO - Iter [5150/8000]	lr: 8.553e-06, eta: 1:02:10, time: 1.053, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2910, decode.acc_seg: 37.9968, loss: 1.2910
2023-02-19 14:41:41,258 - mmseg - INFO - Iter [5200/8000]	lr: 8.403e-06, eta: 1:00:58, time: 1.052, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3412, decode.acc_seg: 36.4365, loss: 1.3412
2023-02-19 14:42:33,812 - mmseg - INFO - Iter [5250/8000]	lr: 8.253e-06, eta: 0:59:46, time: 1.051, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3068, decode.acc_seg: 37.8515, loss: 1.3068
2023-02-19 14:43:26,322 - mmseg - INFO - Iter [5300/8000]	lr: 8.103e-06, eta: 0:58:34, time: 1.050, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3974, decode.acc_seg: 39.1360, loss: 1.3974
2023-02-19 14:44:18,828 - mmseg - INFO - Iter [5350/8000]	lr: 7.953e-06, eta: 0:57:23, time: 1.050, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2908, decode.acc_seg: 38.6143, loss: 1.2908
2023-02-19 14:45:11,308 - mmseg - INFO - Iter [5400/8000]	lr: 7.803e-06, eta: 0:56:12, time: 1.050, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3248, decode.acc_seg: 35.9573, loss: 1.3248
2023-02-19 14:46:03,707 - mmseg - INFO - Iter [5450/8000]	lr: 7.653e-06, eta: 0:55:01, time: 1.048, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3347, decode.acc_seg: 36.2869, loss: 1.3347
2023-02-19 14:46:55,992 - mmseg - INFO - Iter [5500/8000]	lr: 7.503e-06, eta: 0:53:51, time: 1.046, data_time: 0.015, memory: 20583, decode.loss_ce: 1.3262, decode.acc_seg: 38.6727, loss: 1.3262
2023-02-19 14:47:48,297 - mmseg - INFO - Iter [5550/8000]	lr: 7.353e-06, eta: 0:52:41, time: 1.046, data_time: 0.015, memory: 20583, decode.loss_ce: 1.3299, decode.acc_seg: 39.2261, loss: 1.3299
2023-02-19 14:48:40,639 - mmseg - INFO - Iter [5600/8000]	lr: 7.203e-06, eta: 0:51:31, time: 1.047, data_time: 0.015, memory: 20583, decode.loss_ce: 1.2597, decode.acc_seg: 38.1626, loss: 1.2597
2023-02-19 14:49:35,368 - mmseg - INFO - Iter [5650/8000]	lr: 7.053e-06, eta: 0:50:22, time: 1.095, data_time: 0.063, memory: 20583, decode.loss_ce: 1.2891, decode.acc_seg: 39.2966, loss: 1.2891
2023-02-19 14:50:27,854 - mmseg - INFO - Iter [5700/8000]	lr: 6.903e-06, eta: 0:49:13, time: 1.050, data_time: 0.017, memory: 20583, decode.loss_ce: 1.3252, decode.acc_seg: 37.8486, loss: 1.3252
2023-02-19 14:51:20,254 - mmseg - INFO - Iter [5750/8000]	lr: 6.753e-06, eta: 0:48:04, time: 1.048, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2449, decode.acc_seg: 39.0574, loss: 1.2449
2023-02-19 14:52:12,662 - mmseg - INFO - Iter [5800/8000]	lr: 6.603e-06, eta: 0:46:56, time: 1.048, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2945, decode.acc_seg: 38.6543, loss: 1.2945
2023-02-19 14:53:05,038 - mmseg - INFO - Iter [5850/8000]	lr: 6.453e-06, eta: 0:45:48, time: 1.047, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3231, decode.acc_seg: 37.5206, loss: 1.3231
2023-02-19 14:53:57,446 - mmseg - INFO - Iter [5900/8000]	lr: 6.303e-06, eta: 0:44:40, time: 1.048, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2778, decode.acc_seg: 37.7995, loss: 1.2778
2023-02-19 14:54:49,925 - mmseg - INFO - Iter [5950/8000]	lr: 6.153e-06, eta: 0:43:32, time: 1.050, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3437, decode.acc_seg: 39.8149, loss: 1.3437
2023-02-19 14:55:42,422 - mmseg - INFO - Exp name: maskclip_plus_vit16_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 14:55:42,422 - mmseg - INFO - Iter [6000/8000]	lr: 6.003e-06, eta: 0:42:24, time: 1.050, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3363, decode.acc_seg: 38.8673, loss: 1.3363
2023-02-19 14:56:34,884 - mmseg - INFO - Iter [6050/8000]	lr: 5.853e-06, eta: 0:41:17, time: 1.049, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2595, decode.acc_seg: 37.9244, loss: 1.2595
2023-02-19 14:57:27,359 - mmseg - INFO - Iter [6100/8000]	lr: 5.703e-06, eta: 0:40:10, time: 1.049, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2864, decode.acc_seg: 38.8668, loss: 1.2864
2023-02-19 14:58:19,890 - mmseg - INFO - Iter [6150/8000]	lr: 5.553e-06, eta: 0:39:04, time: 1.051, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3809, decode.acc_seg: 39.2487, loss: 1.3809
2023-02-19 14:59:12,481 - mmseg - INFO - Iter [6200/8000]	lr: 5.403e-06, eta: 0:37:57, time: 1.052, data_time: 0.017, memory: 20583, decode.loss_ce: 1.3056, decode.acc_seg: 37.7124, loss: 1.3056
2023-02-19 15:00:07,310 - mmseg - INFO - Iter [6250/8000]	lr: 5.253e-06, eta: 0:36:51, time: 1.097, data_time: 0.063, memory: 20583, decode.loss_ce: 1.3132, decode.acc_seg: 37.7562, loss: 1.3132
2023-02-19 15:00:59,723 - mmseg - INFO - Iter [6300/8000]	lr: 5.103e-06, eta: 0:35:45, time: 1.048, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3064, decode.acc_seg: 38.4619, loss: 1.3064
2023-02-19 15:01:52,165 - mmseg - INFO - Iter [6350/8000]	lr: 4.953e-06, eta: 0:34:39, time: 1.049, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2844, decode.acc_seg: 38.5050, loss: 1.2844
2023-02-19 15:02:44,657 - mmseg - INFO - Iter [6400/8000]	lr: 4.803e-06, eta: 0:33:34, time: 1.050, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3403, decode.acc_seg: 40.0335, loss: 1.3403
2023-02-19 15:03:37,294 - mmseg - INFO - Iter [6450/8000]	lr: 4.653e-06, eta: 0:32:28, time: 1.053, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2667, decode.acc_seg: 37.8264, loss: 1.2667
2023-02-19 15:04:29,827 - mmseg - INFO - Iter [6500/8000]	lr: 4.503e-06, eta: 0:31:23, time: 1.051, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2340, decode.acc_seg: 38.4497, loss: 1.2340
2023-02-19 15:05:22,314 - mmseg - INFO - Iter [6550/8000]	lr: 4.353e-06, eta: 0:30:18, time: 1.050, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2534, decode.acc_seg: 37.8107, loss: 1.2534
2023-02-19 15:06:14,898 - mmseg - INFO - Iter [6600/8000]	lr: 4.203e-06, eta: 0:29:13, time: 1.052, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2136, decode.acc_seg: 37.2839, loss: 1.2136
2023-02-19 15:07:07,426 - mmseg - INFO - Iter [6650/8000]	lr: 4.053e-06, eta: 0:28:09, time: 1.051, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3285, decode.acc_seg: 39.0778, loss: 1.3285
2023-02-19 15:07:59,963 - mmseg - INFO - Iter [6700/8000]	lr: 3.903e-06, eta: 0:27:04, time: 1.051, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2271, decode.acc_seg: 37.0029, loss: 1.2271
2023-02-19 15:08:52,581 - mmseg - INFO - Iter [6750/8000]	lr: 3.753e-06, eta: 0:26:00, time: 1.052, data_time: 0.017, memory: 20583, decode.loss_ce: 1.2564, decode.acc_seg: 37.0435, loss: 1.2564
2023-02-19 15:09:45,226 - mmseg - INFO - Iter [6800/8000]	lr: 3.603e-06, eta: 0:24:56, time: 1.053, data_time: 0.017, memory: 20583, decode.loss_ce: 1.2913, decode.acc_seg: 38.2241, loss: 1.2913
2023-02-19 15:10:37,846 - mmseg - INFO - Iter [6850/8000]	lr: 3.453e-06, eta: 0:23:52, time: 1.052, data_time: 0.017, memory: 20583, decode.loss_ce: 1.3403, decode.acc_seg: 39.1125, loss: 1.3403
2023-02-19 15:11:32,575 - mmseg - INFO - Iter [6900/8000]	lr: 3.303e-06, eta: 0:22:48, time: 1.095, data_time: 0.062, memory: 20583, decode.loss_ce: 1.2752, decode.acc_seg: 38.7436, loss: 1.2752
2023-02-19 15:12:25,091 - mmseg - INFO - Iter [6950/8000]	lr: 3.153e-06, eta: 0:21:44, time: 1.050, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3061, decode.acc_seg: 37.9797, loss: 1.3061
2023-02-19 15:13:17,562 - mmseg - INFO - Exp name: maskclip_plus_vit16_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 15:13:17,562 - mmseg - INFO - Iter [7000/8000]	lr: 3.003e-06, eta: 0:20:41, time: 1.049, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2413, decode.acc_seg: 39.5519, loss: 1.2413
2023-02-19 15:14:09,973 - mmseg - INFO - Iter [7050/8000]	lr: 2.853e-06, eta: 0:19:38, time: 1.048, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3053, decode.acc_seg: 37.4224, loss: 1.3053
2023-02-19 15:15:02,483 - mmseg - INFO - Iter [7100/8000]	lr: 2.703e-06, eta: 0:18:34, time: 1.050, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2858, decode.acc_seg: 37.7598, loss: 1.2858
2023-02-19 15:15:54,933 - mmseg - INFO - Iter [7150/8000]	lr: 2.553e-06, eta: 0:17:31, time: 1.049, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2943, decode.acc_seg: 37.8720, loss: 1.2943
2023-02-19 15:16:47,402 - mmseg - INFO - Iter [7200/8000]	lr: 2.403e-06, eta: 0:16:28, time: 1.049, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2936, decode.acc_seg: 37.9693, loss: 1.2936
2023-02-19 15:17:39,984 - mmseg - INFO - Iter [7250/8000]	lr: 2.253e-06, eta: 0:15:26, time: 1.052, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2648, decode.acc_seg: 37.3509, loss: 1.2648
2023-02-19 15:18:32,555 - mmseg - INFO - Iter [7300/8000]	lr: 2.103e-06, eta: 0:14:23, time: 1.051, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3256, decode.acc_seg: 39.3469, loss: 1.3256
2023-02-19 15:19:25,135 - mmseg - INFO - Iter [7350/8000]	lr: 1.953e-06, eta: 0:13:21, time: 1.052, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2536, decode.acc_seg: 37.0554, loss: 1.2536
2023-02-19 15:20:17,918 - mmseg - INFO - Iter [7400/8000]	lr: 1.803e-06, eta: 0:12:18, time: 1.056, data_time: 0.019, memory: 20583, decode.loss_ce: 1.2849, decode.acc_seg: 38.8636, loss: 1.2849
2023-02-19 15:21:10,649 - mmseg - INFO - Iter [7450/8000]	lr: 1.653e-06, eta: 0:11:16, time: 1.055, data_time: 0.018, memory: 20583, decode.loss_ce: 1.2753, decode.acc_seg: 38.2473, loss: 1.2753
2023-02-19 15:22:05,658 - mmseg - INFO - Iter [7500/8000]	lr: 1.503e-06, eta: 0:10:14, time: 1.100, data_time: 0.064, memory: 20583, decode.loss_ce: 1.3038, decode.acc_seg: 39.3338, loss: 1.3038
2023-02-19 15:22:58,302 - mmseg - INFO - Iter [7550/8000]	lr: 1.353e-06, eta: 0:09:12, time: 1.053, data_time: 0.017, memory: 20583, decode.loss_ce: 1.2990, decode.acc_seg: 37.4302, loss: 1.2990
2023-02-19 15:23:50,919 - mmseg - INFO - Iter [7600/8000]	lr: 1.203e-06, eta: 0:08:10, time: 1.052, data_time: 0.017, memory: 20583, decode.loss_ce: 1.2948, decode.acc_seg: 39.9094, loss: 1.2948
2023-02-19 15:24:43,539 - mmseg - INFO - Iter [7650/8000]	lr: 1.053e-06, eta: 0:07:08, time: 1.052, data_time: 0.017, memory: 20583, decode.loss_ce: 1.2964, decode.acc_seg: 38.6735, loss: 1.2964
2023-02-19 15:25:36,325 - mmseg - INFO - Iter [7700/8000]	lr: 9.030e-07, eta: 0:06:07, time: 1.056, data_time: 0.018, memory: 20583, decode.loss_ce: 1.3365, decode.acc_seg: 38.7507, loss: 1.3365
2023-02-19 15:26:29,086 - mmseg - INFO - Iter [7750/8000]	lr: 7.530e-07, eta: 0:05:05, time: 1.055, data_time: 0.018, memory: 20583, decode.loss_ce: 1.2722, decode.acc_seg: 39.8825, loss: 1.2722
2023-02-19 15:27:21,663 - mmseg - INFO - Iter [7800/8000]	lr: 6.030e-07, eta: 0:04:04, time: 1.052, data_time: 0.017, memory: 20583, decode.loss_ce: 1.2726, decode.acc_seg: 37.7319, loss: 1.2726
2023-02-19 15:28:14,237 - mmseg - INFO - Iter [7850/8000]	lr: 4.530e-07, eta: 0:03:03, time: 1.051, data_time: 0.017, memory: 20583, decode.loss_ce: 1.2674, decode.acc_seg: 39.2603, loss: 1.2674
2023-02-19 15:29:07,030 - mmseg - INFO - Iter [7900/8000]	lr: 3.030e-07, eta: 0:02:02, time: 1.056, data_time: 0.018, memory: 20583, decode.loss_ce: 1.2567, decode.acc_seg: 38.5634, loss: 1.2567
2023-02-19 15:29:59,396 - mmseg - INFO - Iter [7950/8000]	lr: 1.530e-07, eta: 0:01:00, time: 1.047, data_time: 0.016, memory: 20583, decode.loss_ce: 1.2525, decode.acc_seg: 38.5845, loss: 1.2525
2023-02-19 15:30:51,711 - mmseg - INFO - Saving checkpoint at 8000 iterations
2023-02-19 15:30:54,000 - mmseg - INFO - Exp name: maskclip_plus_vit16_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 15:30:54,000 - mmseg - INFO - Iter [8000/8000]	lr: 3.000e-09, eta: 0:00:00, time: 1.092, data_time: 0.016, memory: 20583, decode.loss_ce: 1.3601, decode.acc_seg: 38.4324, loss: 1.3601
2023-02-19 15:52:25,430 - mmseg - INFO - per class results:
2023-02-19 15:52:25,434 - mmseg - INFO - 
+-------------+-------+-------+-------+
|    Class    |  IoU  |  Acc  |  Prec |
+-------------+-------+-------+-------+
|  aeroplane  | 44.84 | 94.99 | 45.92 |
|     bag     | 14.63 | 28.06 | 23.41 |
|     bed     | 10.56 | 51.52 | 11.73 |
|  bedclothes | 13.09 | 16.01 | 41.82 |
|    bench    |  0.5  |  0.5  | 44.26 |
|   bicycle   | 52.06 | 87.84 |  56.1 |
|     bird    | 67.19 | 94.64 | 69.84 |
|     boat    | 38.67 | 85.63 | 41.35 |
|     book    | 27.08 | 28.99 | 80.42 |
|    bottle   |  53.5 | 71.72 | 67.81 |
|   building  | 14.01 | 14.41 | 83.42 |
|     bus     | 77.06 | 86.84 | 87.25 |
|   cabinet   |  9.76 | 28.93 | 12.84 |
|     car     |  67.2 | 87.75 | 74.16 |
|     cat     | 82.48 | 93.25 | 87.72 |
|   ceiling   | 31.81 |  69.4 | 36.99 |
|    chair    | 30.91 | 42.11 | 53.74 |
|    cloth    |  6.39 | 31.55 |  7.42 |
|   computer  |  9.63 | 34.65 | 11.77 |
|     cow     | 69.82 | 91.49 | 74.67 |
|     cup     |  6.6  |  22.3 |  8.57 |
|   curtain   | 35.33 | 52.01 | 52.42 |
|     dog     |  77.7 | 88.88 | 86.07 |
|     door    |  9.89 | 58.34 | 10.64 |
|    fence    | 22.62 | 26.83 | 59.08 |
|    floor    | 54.87 | 72.28 |  69.5 |
|    flower   | 18.24 | 41.17 | 24.66 |
|     food    |  7.04 | 89.47 |  7.1  |
|    grass    | 59.88 | 72.18 | 77.84 |
|    ground   | 22.87 | 38.92 | 35.67 |
|    horse    | 61.86 | 93.41 | 64.68 |
|   keyboard  | 69.14 | 74.79 | 90.15 |
|    light    |  12.9 | 67.82 | 13.74 |
|  motorbike  | 59.36 | 74.28 | 74.72 |
|   mountain  | 32.85 | 48.01 | 50.99 |
|    mouse    | 10.92 | 54.29 | 12.03 |
|    person   |  42.4 | 47.49 | 79.83 |
|    plate    |  4.36 |  9.6  |  7.39 |
|   platform  |  0.0  |  0.0  |  0.0  |
| pottedplant | 34.76 | 77.99 | 38.54 |
|     road    | 39.78 | 54.69 | 59.34 |
|     rock    |  30.5 | 64.39 | 36.68 |
|    sheep    | 71.58 | 86.28 | 80.77 |
|   shelves   |  6.26 |  6.68 | 50.29 |
|   sidewalk  | 10.32 | 71.05 | 10.77 |
|     sign    | 29.03 | 60.35 | 35.87 |
|     sky     | 59.41 | 59.79 | 98.97 |
|     snow    | 53.35 | 75.64 | 64.43 |
|     sofa    | 39.34 | 47.81 | 68.95 |
|    table    | 15.71 | 46.99 | 19.09 |
|    track    |  0.0  |  0.0  |  0.0  |
|    train    | 47.34 | 87.83 | 50.67 |
|     tree    | 43.12 | 46.99 | 83.95 |
|    truck    | 12.05 | 50.97 | 13.63 |
|  tvmonitor  | 40.72 | 48.74 | 71.21 |
|     wall    | 32.58 | 40.61 | 62.24 |
|    water    | 70.17 | 78.19 | 87.25 |
|    window   | 25.67 | 34.23 | 50.66 |
|     wood    |  7.06 | 45.57 |  7.71 |
+-------------+-------+-------+-------+
2023-02-19 15:52:25,434 - mmseg - INFO - Summary:
2023-02-19 15:52:25,434 - mmseg - INFO - 
+-------+-------+-------+-------+
|  aAcc |  mIoU |  mAcc | mPrec |
+-------+-------+-------+-------+
| 56.59 | 33.88 | 55.21 | 47.95 |
+-------+-------+-------+-------+
2023-02-19 15:52:25,453 - mmseg - INFO - Exp name: maskclip_plus_vit16_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 15:52:25,453 - mmseg - INFO - Iter(val) [5104]	aAcc: 0.5659, mIoU: 0.3388, mAcc: 0.5521, mPrec: 0.4795, IoU.aeroplane: 0.4484, IoU.bag: 0.1463, IoU.bed: 0.1056, IoU.bedclothes: 0.1309, IoU.bench: 0.0050, IoU.bicycle: 0.5206, IoU.bird: 0.6719, IoU.boat: 0.3867, IoU.book: 0.2708, IoU.bottle: 0.5350, IoU.building: 0.1401, IoU.bus: 0.7706, IoU.cabinet: 0.0976, IoU.car: 0.6720, IoU.cat: 0.8248, IoU.ceiling: 0.3181, IoU.chair: 0.3091, IoU.cloth: 0.0639, IoU.computer: 0.0963, IoU.cow: 0.6982, IoU.cup: 0.0660, IoU.curtain: 0.3533, IoU.dog: 0.7770, IoU.door: 0.0989, IoU.fence: 0.2262, IoU.floor: 0.5487, IoU.flower: 0.1824, IoU.food: 0.0704, IoU.grass: 0.5988, IoU.ground: 0.2287, IoU.horse: 0.6186, IoU.keyboard: 0.6914, IoU.light: 0.1290, IoU.motorbike: 0.5936, IoU.mountain: 0.3285, IoU.mouse: 0.1092, IoU.person: 0.4240, IoU.plate: 0.0436, IoU.platform: 0.0000, IoU.pottedplant: 0.3476, IoU.road: 0.3978, IoU.rock: 0.3050, IoU.sheep: 0.7158, IoU.shelves: 0.0626, IoU.sidewalk: 0.1032, IoU.sign: 0.2903, IoU.sky: 0.5941, IoU.snow: 0.5335, IoU.sofa: 0.3934, IoU.table: 0.1571, IoU.track: 0.0000, IoU.train: 0.4734, IoU.tree: 0.4312, IoU.truck: 0.1205, IoU.tvmonitor: 0.4072, IoU.wall: 0.3258, IoU.water: 0.7017, IoU.window: 0.2567, IoU.wood: 0.0706, Acc.aeroplane: 0.9499, Acc.bag: 0.2806, Acc.bed: 0.5152, Acc.bedclothes: 0.1601, Acc.bench: 0.0050, Acc.bicycle: 0.8784, Acc.bird: 0.9464, Acc.boat: 0.8563, Acc.book: 0.2899, Acc.bottle: 0.7172, Acc.building: 0.1441, Acc.bus: 0.8684, Acc.cabinet: 0.2893, Acc.car: 0.8775, Acc.cat: 0.9325, Acc.ceiling: 0.6940, Acc.chair: 0.4211, Acc.cloth: 0.3155, Acc.computer: 0.3465, Acc.cow: 0.9149, Acc.cup: 0.2230, Acc.curtain: 0.5201, Acc.dog: 0.8888, Acc.door: 0.5834, Acc.fence: 0.2683, Acc.floor: 0.7228, Acc.flower: 0.4117, Acc.food: 0.8947, Acc.grass: 0.7218, Acc.ground: 0.3892, Acc.horse: 0.9341, Acc.keyboard: 0.7479, Acc.light: 0.6782, Acc.motorbike: 0.7428, Acc.mountain: 0.4801, Acc.mouse: 0.5429, Acc.person: 0.4749, Acc.plate: 0.0960, Acc.platform: 0.0000, Acc.pottedplant: 0.7799, Acc.road: 0.5469, Acc.rock: 0.6439, Acc.sheep: 0.8628, Acc.shelves: 0.0668, Acc.sidewalk: 0.7105, Acc.sign: 0.6035, Acc.sky: 0.5979, Acc.snow: 0.7564, Acc.sofa: 0.4781, Acc.table: 0.4699, Acc.track: 0.0000, Acc.train: 0.8783, Acc.tree: 0.4699, Acc.truck: 0.5097, Acc.tvmonitor: 0.4874, Acc.wall: 0.4061, Acc.water: 0.7819, Acc.window: 0.3423, Acc.wood: 0.4557, Prec.aeroplane: 0.4592, Prec.bag: 0.2341, Prec.bed: 0.1173, Prec.bedclothes: 0.4182, Prec.bench: 0.4426, Prec.bicycle: 0.5610, Prec.bird: 0.6984, Prec.boat: 0.4135, Prec.book: 0.8042, Prec.bottle: 0.6781, Prec.building: 0.8342, Prec.bus: 0.8725, Prec.cabinet: 0.1284, Prec.car: 0.7416, Prec.cat: 0.8772, Prec.ceiling: 0.3699, Prec.chair: 0.5374, Prec.cloth: 0.0742, Prec.computer: 0.1177, Prec.cow: 0.7467, Prec.cup: 0.0857, Prec.curtain: 0.5242, Prec.dog: 0.8607, Prec.door: 0.1064, Prec.fence: 0.5908, Prec.floor: 0.6950, Prec.flower: 0.2466, Prec.food: 0.0710, Prec.grass: 0.7784, Prec.ground: 0.3567, Prec.horse: 0.6468, Prec.keyboard: 0.9015, Prec.light: 0.1374, Prec.motorbike: 0.7472, Prec.mountain: 0.5099, Prec.mouse: 0.1203, Prec.person: 0.7983, Prec.plate: 0.0739, Prec.platform: 0.0000, Prec.pottedplant: 0.3854, Prec.road: 0.5934, Prec.rock: 0.3668, Prec.sheep: 0.8077, Prec.shelves: 0.5029, Prec.sidewalk: 0.1077, Prec.sign: 0.3587, Prec.sky: 0.9897, Prec.snow: 0.6443, Prec.sofa: 0.6895, Prec.table: 0.1909, Prec.track: 0.0000, Prec.train: 0.5067, Prec.tree: 0.8395, Prec.truck: 0.1363, Prec.tvmonitor: 0.7121, Prec.wall: 0.6224, Prec.water: 0.8725, Prec.window: 0.5066, Prec.wood: 0.0771
