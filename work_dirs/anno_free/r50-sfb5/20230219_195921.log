2023-02-19 19:59:21,224 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.16 (default, Jan 17 2023, 23:13:24) [GCC 11.2.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 3090
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.2, V11.2.152
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.9.1+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.10.1+cu111
OpenCV: 4.7.0
MMCV: 1.5.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
MMSegmentation: 0.20.2+e20a1c2
------------------------------------------------------------

2023-02-19 19:59:21,225 - mmseg - INFO - Distributed training: False
2023-02-19 19:59:22,758 - mmseg - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='EncoderDecoder',
    pretrained='pretrain/mit_b5_weight.pth',
    backbone=dict(
        type='MixVisionTransformer',
        in_channels=3,
        embed_dims=64,
        num_stages=4,
        num_layers=[3, 6, 40, 3],
        num_heads=[1, 2, 5, 8],
        patch_sizes=[7, 3, 3, 3],
        sr_ratios=[8, 4, 2, 1],
        out_indices=(0, 1, 2, 3),
        mlp_ratio=4,
        qkv_bias=True,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.1),
    decode_head=dict(
        type='MaskClipPlusSegformerHead',
        vit=False,
        in_channels=2048,
        channels=1024,
        num_classes=59,
        dropout_ratio=0,
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
        decode_module_cfg=dict(
            type='SegformerHead',
            in_channels=[64, 128, 320, 512],
            in_index=[0, 1, 2, 3],
            channels=1024,
            dropout_ratio=0.1,
            num_classes=59,
            norm_cfg=dict(type='SyncBN', requires_grad=True),
            align_corners=False),
        text_categories=59,
        text_channels=1024,
        text_embeddings_path='pretrain/context_RN50_clip_text.pth',
        cls_bg=False,
        norm_feat=False,
        clip_unlabeled_cats=[
            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,
            19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
            36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52,
            53, 54, 55, 56, 57, 58
        ],
        clip_cfg=dict(
            type='ResNetClip',
            depth=50,
            norm_cfg=dict(type='SyncBN', requires_grad=True),
            contract_dilation=True),
        clip_weights_path='pretrain/RN50_clip_weights.pth',
        reset_counter=True,
        start_clip_guided=(1, -1),
        start_self_train=(-1, -1)),
    feed_img_to_decode_head=True,
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))
dataset_type = 'PascalContextDataset59'
data_root = 'data/VOCdevkit/VOC2010/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
img_scale = (520, 520)
crop_size = (480, 480)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='LoadAnnotations',
        reduce_zero_label=True,
        suppress_labels=[
            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,
            19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
            36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52,
            53, 54, 55, 56, 57, 58
        ]),
    dict(type='Resize', img_scale=(520, 520), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(480, 480), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(480, 480), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(520, 520),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=6,
    workers_per_gpu=4,
    train=dict(
        type='PascalContextDataset59',
        data_root='data/VOCdevkit/VOC2010/',
        img_dir='JPEGImages',
        ann_dir='SegmentationClassContext',
        split='ImageSets/SegmentationContext/train.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='LoadAnnotations',
                reduce_zero_label=True,
                suppress_labels=[
                    0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
                    17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,
                    32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46,
                    47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58
                ]),
            dict(type='Resize', img_scale=(520, 520), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(480, 480), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(480, 480), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ]),
    val=dict(
        type='PascalContextDataset59',
        data_root='data/VOCdevkit/VOC2010/',
        img_dir='JPEGImages',
        ann_dir='SegmentationClassContext',
        split='ImageSets/SegmentationContext/val.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(520, 520),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='PascalContextDataset59',
        data_root='data/VOCdevkit/VOC2010/',
        img_dir='JPEGImages',
        ann_dir='SegmentationClassContext',
        split='ImageSets/SegmentationContext/val.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(520, 520),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
optimizer = dict(
    type='AdamW',
    lr=6e-05,
    betas=(0.9, 0.999),
    weight_decay=0.004,
    paramwise_cfg=dict(
        custom_keys=dict(
            pos_block=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0),
            head=dict(lr_mult=10.0))))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=1500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=8000)
checkpoint_config = dict(by_epoch=False, interval=4000)
evaluation = dict(interval=4000, metric='mIoU', pre_eval=True)
suppress_labels = [
    0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,
    21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,
    40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58
]
find_unused_parameters = True
work_dir = 'work_dirs/anno_free/r50-sfb5'
gpu_ids = range(0, 1)
auto_resume = False

2023-02-19 19:59:22,759 - mmseg - INFO - Set random seed to 1344807963, deterministic: False
2023-02-19 19:59:27,698 - mmseg - INFO - initialize MixVisionTransformer with init_cfg {'type': 'Pretrained', 'checkpoint': 'pretrain/mit_b5_weight.pth'}
2023-02-19 19:59:29,607 - mmseg - INFO - initialize SegformerHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
2023-02-19 19:59:29,828 - mmseg - INFO - initialize ResNetClip with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-19 19:59:30,191 - mmseg - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2023-02-19 19:59:30,192 - mmseg - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2023-02-19 19:59:30,193 - mmseg - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2023-02-19 19:59:30,199 - mmseg - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2023-02-19 19:59:30,207 - mmseg - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2023-02-19 19:59:30,209 - mmseg - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2023-02-19 19:59:30,210 - mmseg - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2023-02-19 19:59:30,214 - mmseg - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2023-02-19 19:59:30,215 - mmseg - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2023-02-19 19:59:30,217 - mmseg - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2023-02-19 19:59:30,218 - mmseg - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2023-02-19 19:59:30,306 - mmseg - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2023-02-19 19:59:30,309 - mmseg - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2023-02-19 19:59:30,313 - mmseg - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2023-02-19 19:59:30,314 - mmseg - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2023-02-19 19:59:30,316 - mmseg - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2023-02-19 19:59:30,505 - mmseg - INFO - Loaded text embeddings from pretrain/context_RN50_clip_text.pth
2023-02-19 19:59:30,751 - mmseg - INFO - Loaded clip weights from pretrain/RN50_clip_weights.pth
Name of parameter - Initialization information

backbone.layers.0.0.projection.weight - torch.Size([64, 3, 7, 7]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.0.projection.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.0.norm.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.0.norm.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.norm1.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.norm1.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.attn.attn.in_proj_weight - torch.Size([192, 64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.attn.attn.in_proj_bias - torch.Size([192]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.attn.attn.out_proj.weight - torch.Size([64, 64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.attn.attn.out_proj.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.attn.sr.weight - torch.Size([64, 64, 8, 8]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.attn.sr.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.attn.norm.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.attn.norm.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.norm2.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.norm2.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.ffn.layers.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.ffn.layers.0.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.ffn.layers.1.weight - torch.Size([256, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.ffn.layers.1.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.ffn.layers.4.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.0.ffn.layers.4.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.norm1.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.norm1.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.attn.attn.in_proj_weight - torch.Size([192, 64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.attn.attn.in_proj_bias - torch.Size([192]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.attn.attn.out_proj.weight - torch.Size([64, 64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.attn.attn.out_proj.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.attn.sr.weight - torch.Size([64, 64, 8, 8]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.attn.sr.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.attn.norm.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.attn.norm.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.norm2.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.norm2.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.ffn.layers.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.ffn.layers.0.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.ffn.layers.1.weight - torch.Size([256, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.ffn.layers.1.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.ffn.layers.4.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.1.ffn.layers.4.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.norm1.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.norm1.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.attn.attn.in_proj_weight - torch.Size([192, 64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.attn.attn.in_proj_bias - torch.Size([192]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.attn.attn.out_proj.weight - torch.Size([64, 64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.attn.attn.out_proj.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.attn.sr.weight - torch.Size([64, 64, 8, 8]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.attn.sr.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.attn.norm.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.attn.norm.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.norm2.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.norm2.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.ffn.layers.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.ffn.layers.0.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.ffn.layers.1.weight - torch.Size([256, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.ffn.layers.1.bias - torch.Size([256]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.ffn.layers.4.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.1.2.ffn.layers.4.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.2.weight - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.0.2.bias - torch.Size([64]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.0.projection.weight - torch.Size([128, 64, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.0.projection.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.0.norm.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.0.norm.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.norm1.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.norm1.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.attn.attn.in_proj_weight - torch.Size([384, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.attn.attn.in_proj_bias - torch.Size([384]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.attn.attn.out_proj.weight - torch.Size([128, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.attn.attn.out_proj.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.attn.sr.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.attn.norm.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.attn.norm.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.norm2.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.norm2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.ffn.layers.0.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.ffn.layers.1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.0.ffn.layers.4.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.norm1.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.norm1.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.attn.attn.in_proj_weight - torch.Size([384, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.attn.attn.in_proj_bias - torch.Size([384]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.attn.attn.out_proj.weight - torch.Size([128, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.attn.attn.out_proj.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.attn.sr.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.attn.norm.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.attn.norm.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.norm2.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.norm2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.ffn.layers.0.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.ffn.layers.1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.1.ffn.layers.4.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.norm1.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.norm1.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.attn.attn.in_proj_weight - torch.Size([384, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.attn.attn.in_proj_bias - torch.Size([384]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.attn.attn.out_proj.weight - torch.Size([128, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.attn.attn.out_proj.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.attn.sr.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.attn.norm.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.attn.norm.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.norm2.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.norm2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.ffn.layers.0.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.ffn.layers.1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.2.ffn.layers.4.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.norm1.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.norm1.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.attn.attn.in_proj_weight - torch.Size([384, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.attn.attn.in_proj_bias - torch.Size([384]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.attn.attn.out_proj.weight - torch.Size([128, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.attn.attn.out_proj.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.attn.sr.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.attn.norm.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.attn.norm.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.norm2.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.norm2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.ffn.layers.0.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.ffn.layers.1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.3.ffn.layers.4.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.norm1.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.norm1.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.attn.attn.in_proj_weight - torch.Size([384, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.attn.attn.in_proj_bias - torch.Size([384]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.attn.attn.out_proj.weight - torch.Size([128, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.attn.attn.out_proj.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.attn.sr.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.attn.norm.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.attn.norm.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.norm2.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.norm2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.ffn.layers.0.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.ffn.layers.1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.4.ffn.layers.4.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.norm1.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.norm1.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.attn.attn.in_proj_weight - torch.Size([384, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.attn.attn.in_proj_bias - torch.Size([384]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.attn.attn.out_proj.weight - torch.Size([128, 128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.attn.attn.out_proj.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.attn.sr.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.attn.norm.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.attn.norm.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.norm2.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.norm2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.ffn.layers.0.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.ffn.layers.1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.1.5.ffn.layers.4.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.2.weight - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.1.2.bias - torch.Size([128]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.0.projection.weight - torch.Size([320, 128, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.0.projection.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.0.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.0.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.0.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.1.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.2.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.3.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.4.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.5.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.6.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.7.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.8.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.9.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.10.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.11.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.12.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.13.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.14.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.15.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.16.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.17.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.18.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.19.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.20.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.21.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.22.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.23.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.24.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.25.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.26.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.27.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.28.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.29.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.30.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.31.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.32.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.33.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.34.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.35.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.36.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.37.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.38.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.norm1.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.norm1.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.attn.attn.in_proj_weight - torch.Size([960, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.attn.attn.in_proj_bias - torch.Size([960]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.attn.attn.out_proj.weight - torch.Size([320, 320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.attn.attn.out_proj.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.attn.sr.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.attn.norm.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.attn.norm.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.norm2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.norm2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.ffn.layers.0.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.ffn.layers.1.bias - torch.Size([1280]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.1.39.ffn.layers.4.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.2.weight - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.2.2.bias - torch.Size([320]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.0.projection.weight - torch.Size([512, 320, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.0.projection.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.0.norm.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.0.norm.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.attn.attn.in_proj_weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.attn.attn.in_proj_bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.attn.attn.out_proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.attn.attn.out_proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.ffn.layers.0.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.ffn.layers.0.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.ffn.layers.1.weight - torch.Size([2048, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.ffn.layers.1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.ffn.layers.4.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.0.ffn.layers.4.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.attn.attn.in_proj_weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.attn.attn.in_proj_bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.attn.attn.out_proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.attn.attn.out_proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.ffn.layers.0.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.ffn.layers.0.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.ffn.layers.1.weight - torch.Size([2048, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.ffn.layers.1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.ffn.layers.4.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.1.ffn.layers.4.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.norm1.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.norm1.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.attn.attn.in_proj_weight - torch.Size([1536, 512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.attn.attn.in_proj_bias - torch.Size([1536]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.attn.attn.out_proj.weight - torch.Size([512, 512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.attn.attn.out_proj.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.norm2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.norm2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.ffn.layers.0.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.ffn.layers.0.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.ffn.layers.1.weight - torch.Size([2048, 1, 3, 3]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.ffn.layers.1.bias - torch.Size([2048]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.ffn.layers.4.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.1.2.ffn.layers.4.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.2.weight - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

backbone.layers.3.2.bias - torch.Size([512]): 
PretrainedInit: load from pretrain/mit_b5_weight.pth 

decode_head.decode_module.conv_seg.weight - torch.Size([59, 1024, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.decode_module.conv_seg.bias - torch.Size([59]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.decode_module.convs.0.conv.weight - torch.Size([1024, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.0.bn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.0.bn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.1.conv.weight - torch.Size([1024, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.1.bn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.1.bn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.2.conv.weight - torch.Size([1024, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.2.bn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.2.bn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.3.conv.weight - torch.Size([1024, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.3.bn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.convs.3.bn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.fusion_conv.conv.weight - torch.Size([1024, 4096, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

decode_head.decode_module.fusion_conv.bn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.decode_module.fusion_conv.bn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.clip.stem.0.weight - torch.Size([32, 3, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.stem.1.weight - torch.Size([32]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.stem.1.bias - torch.Size([32]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.stem.3.weight - torch.Size([32, 32, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.stem.4.weight - torch.Size([32]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.stem.4.bias - torch.Size([32]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.stem.6.weight - torch.Size([64, 32, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.stem.7.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.stem.7.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.0.bn1.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.0.bn1.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.0.bn2.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.0.bn2.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.0.bn3.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.0.bn3.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.0.downsample.1.weight - torch.Size([256, 64, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.0.downsample.2.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.0.downsample.2.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.1.bn1.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.1.bn1.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.1.bn2.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.1.bn2.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.1.bn3.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.1.bn3.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.2.bn1.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.2.bn1.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.2.bn2.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.2.bn2.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.2.bn3.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer1.2.bn3.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.0.bn1.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.0.bn1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.0.bn2.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.0.bn2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.0.bn3.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.0.bn3.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.0.downsample.1.weight - torch.Size([512, 256, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.0.downsample.2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.0.downsample.2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.1.bn1.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.1.bn1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.1.bn2.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.1.bn2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.1.bn3.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.1.bn3.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.2.bn1.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.2.bn1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.2.bn2.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.2.bn2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.2.bn3.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.2.bn3.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.3.bn1.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.3.bn1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.3.bn2.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.3.bn2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.3.bn3.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer2.3.bn3.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.0.bn1.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.0.bn1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.0.bn2.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.0.bn2.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.0.bn3.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.0.bn3.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.0.downsample.1.weight - torch.Size([1024, 512, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.0.downsample.2.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.0.downsample.2.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.1.bn1.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.1.bn1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.1.bn2.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.1.bn2.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.1.bn3.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.1.bn3.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.2.bn1.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.2.bn1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.2.bn2.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.2.bn2.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.2.bn3.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.2.bn3.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.3.bn1.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.3.bn1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.3.bn2.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.3.bn2.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.3.bn3.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.3.bn3.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.4.bn1.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.4.bn1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.4.bn2.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.4.bn2.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.4.bn3.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.4.bn3.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.5.bn1.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.5.bn1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.5.bn2.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.5.bn2.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.5.bn3.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer3.5.bn3.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.0.bn1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.0.bn1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.0.bn2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.0.bn2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.0.bn3.weight - torch.Size([2048]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.0.bn3.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.0.downsample.1.weight - torch.Size([2048, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.0.downsample.2.weight - torch.Size([2048]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.0.downsample.2.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.1.bn1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.1.bn1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.1.bn2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.1.bn2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.1.bn3.weight - torch.Size([2048]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.1.bn3.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.2.bn1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.2.bn1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.2.bn2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.2.bn2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.2.bn3.weight - torch.Size([2048]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.clip.layer4.2.bn3.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.q_proj.weight - torch.Size([2048, 2048, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.q_proj.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.k_proj.weight - torch.Size([2048, 2048, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.k_proj.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.v_proj.weight - torch.Size([2048, 2048, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.v_proj.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.c_proj.weight - torch.Size([1024, 2048, 1, 1]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  

decode_head.c_proj.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in MaskClipPlusSegformerHead  
2023-02-19 19:59:30,806 - mmseg - INFO - EncoderDecoder(
  (backbone): MixVisionTransformer(
    (layers): ModuleList(
      (0): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
          (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0): TransformerEncoderLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): TransformerEncoderLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): TransformerEncoderLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      )
      (1): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (3): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (4): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (5): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
      (2): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (3): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (4): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (5): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (6): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (7): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (8): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (9): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (10): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (11): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (12): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (13): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (14): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (15): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (16): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (17): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (18): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (19): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (20): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (21): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (22): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (23): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (24): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (25): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (26): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (27): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (28): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (29): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (30): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (31): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (32): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (33): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (34): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (35): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (36): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (37): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (38): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (39): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      )
      (3): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0): TransformerEncoderLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): TransformerEncoderLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): TransformerEncoderLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  init_cfg={'type': 'Pretrained', 'checkpoint': 'pretrain/mit_b5_weight.pth'}
  (decode_head): MaskClipPlusSegformerHead(
    input_transform=None, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss()
    (decode_module): SegformerHead(
      input_transform=multiple_select, ignore_index=255, align_corners=False
      (loss_decode): CrossEntropyLoss()
      (conv_seg): Conv2d(1024, 59, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout2d(p=0.1, inplace=False)
      (convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (1): ConvModule(
          (conv): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (2): ConvModule(
          (conv): Conv2d(320, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (3): ConvModule(
          (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
      )
      (fusion_conv): ConvModule(
        (conv): Conv2d(4096, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
    (clip): ResNetClip(
      (stem): Sequential(
        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): _BatchNormXd(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): _BatchNormXd(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (7): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (8): ReLU(inplace=True)
      )
      (stem_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
      (layer1): ResLayer(
        (0): Bottleneck(
          (avgpool): Identity()
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      )
      (layer2): ResLayer(
        (0): Bottleneck(
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      )
      (layer3): ResLayer(
        (0): Bottleneck(
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      )
      (layer4): ResLayer(
        (0): Bottleneck(
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): _BatchNormXd(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): _BatchNormXd(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): _BatchNormXd(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): _BatchNormXd(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      )
    )
    init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
    (q_proj): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1))
    (k_proj): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1))
    (v_proj): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1))
    (c_proj): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))
  )
)
2023-02-19 19:59:30,999 - mmseg - INFO - Loaded 4996 images
2023-02-19 19:59:33,158 - mmseg - INFO - Loaded 5104 images
2023-02-19 19:59:33,158 - mmseg - INFO - Start running, host: root@workspace-p4cxarvku5qr-0, work_dir: /root/sj/MaskCLIP_SegFormer/work_dirs/anno_free/r50-sfb5
2023-02-19 19:59:33,159 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-02-19 19:59:33,159 - mmseg - INFO - workflow: [('train', 1)], max: 8000 iters
2023-02-19 19:59:33,159 - mmseg - INFO - Checkpoints will be saved to /root/sj/MaskCLIP_SegFormer/work_dirs/anno_free/r50-sfb5 by HardDiskBackend.
2023-02-19 19:59:34,806 - mmseg - INFO - Start clip guided training
2023-02-19 20:01:24,422 - mmseg - INFO - Iter [50/8000]	lr: 1.948e-06, eta: 4:53:07, time: 2.212, data_time: 0.019, memory: 18982, decode.loss_ce: 2.9578, decode.acc_seg: 1.2229, loss: 2.9578
2023-02-19 20:02:12,443 - mmseg - INFO - Iter [100/8000]	lr: 3.911e-06, eta: 3:28:52, time: 0.960, data_time: 0.012, memory: 18982, decode.loss_ce: 2.7762, decode.acc_seg: 5.1137, loss: 2.7762
2023-02-19 20:03:00,559 - mmseg - INFO - Iter [150/8000]	lr: 5.849e-06, eta: 3:00:19, time: 0.962, data_time: 0.012, memory: 18982, decode.loss_ce: 2.7216, decode.acc_seg: 11.5728, loss: 2.7216
2023-02-19 20:03:48,772 - mmseg - INFO - Iter [200/8000]	lr: 7.762e-06, eta: 2:45:43, time: 0.964, data_time: 0.012, memory: 18982, decode.loss_ce: 2.6044, decode.acc_seg: 17.5667, loss: 2.6044
2023-02-19 20:04:36,956 - mmseg - INFO - Iter [250/8000]	lr: 9.650e-06, eta: 2:36:37, time: 0.964, data_time: 0.012, memory: 18982, decode.loss_ce: 2.2763, decode.acc_seg: 22.2669, loss: 2.2763
2023-02-19 20:05:25,226 - mmseg - INFO - Iter [300/8000]	lr: 1.151e-05, eta: 2:30:19, time: 0.965, data_time: 0.013, memory: 18982, decode.loss_ce: 2.1734, decode.acc_seg: 24.3702, loss: 2.1734
2023-02-19 20:06:13,495 - mmseg - INFO - Iter [350/8000]	lr: 1.335e-05, eta: 2:25:35, time: 0.965, data_time: 0.012, memory: 18982, decode.loss_ce: 1.8271, decode.acc_seg: 25.2358, loss: 1.8271
2023-02-19 20:07:01,753 - mmseg - INFO - Iter [400/8000]	lr: 1.516e-05, eta: 2:21:50, time: 0.965, data_time: 0.013, memory: 18982, decode.loss_ce: 1.8326, decode.acc_seg: 27.6417, loss: 1.8326
2023-02-19 20:07:49,999 - mmseg - INFO - Iter [450/8000]	lr: 1.695e-05, eta: 2:18:44, time: 0.965, data_time: 0.012, memory: 18982, decode.loss_ce: 1.7802, decode.acc_seg: 30.1515, loss: 1.7802
2023-02-19 20:08:38,172 - mmseg - INFO - Iter [500/8000]	lr: 1.872e-05, eta: 2:16:05, time: 0.963, data_time: 0.012, memory: 18982, decode.loss_ce: 1.7961, decode.acc_seg: 31.2850, loss: 1.7961
2023-02-19 20:09:26,110 - mmseg - INFO - Iter [550/8000]	lr: 2.045e-05, eta: 2:13:42, time: 0.959, data_time: 0.012, memory: 18982, decode.loss_ce: 1.5480, decode.acc_seg: 30.3794, loss: 1.5480
2023-02-19 20:10:14,256 - mmseg - INFO - Iter [600/8000]	lr: 2.217e-05, eta: 2:11:38, time: 0.963, data_time: 0.012, memory: 18982, decode.loss_ce: 1.6979, decode.acc_seg: 32.7877, loss: 1.6979
2023-02-19 20:11:02,271 - mmseg - INFO - Iter [650/8000]	lr: 2.385e-05, eta: 2:09:44, time: 0.960, data_time: 0.012, memory: 18982, decode.loss_ce: 1.4899, decode.acc_seg: 31.3070, loss: 1.4899
2023-02-19 20:11:50,180 - mmseg - INFO - Iter [700/8000]	lr: 2.552e-05, eta: 2:07:59, time: 0.958, data_time: 0.012, memory: 18982, decode.loss_ce: 1.5317, decode.acc_seg: 32.4061, loss: 1.5317
2023-02-19 20:12:38,205 - mmseg - INFO - Iter [750/8000]	lr: 2.716e-05, eta: 2:06:22, time: 0.960, data_time: 0.012, memory: 18982, decode.loss_ce: 1.4993, decode.acc_seg: 32.2036, loss: 1.4993
2023-02-19 20:13:26,192 - mmseg - INFO - Iter [800/8000]	lr: 2.877e-05, eta: 2:04:51, time: 0.960, data_time: 0.011, memory: 18982, decode.loss_ce: 1.4881, decode.acc_seg: 33.3009, loss: 1.4881
2023-02-19 20:14:16,563 - mmseg - INFO - Iter [850/8000]	lr: 3.036e-05, eta: 2:03:45, time: 1.007, data_time: 0.058, memory: 18982, decode.loss_ce: 1.4035, decode.acc_seg: 33.1425, loss: 1.4035
2023-02-19 20:15:04,440 - mmseg - INFO - Iter [900/8000]	lr: 3.192e-05, eta: 2:02:21, time: 0.958, data_time: 0.012, memory: 18982, decode.loss_ce: 1.4375, decode.acc_seg: 32.4232, loss: 1.4375
2023-02-19 20:15:52,283 - mmseg - INFO - Iter [950/8000]	lr: 3.346e-05, eta: 2:01:01, time: 0.957, data_time: 0.011, memory: 18982, decode.loss_ce: 1.4116, decode.acc_seg: 34.5473, loss: 1.4116
2023-02-19 20:16:40,210 - mmseg - INFO - Exp name: maskclip_plus_r50_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 20:16:40,210 - mmseg - INFO - Iter [1000/8000]	lr: 3.497e-05, eta: 1:59:44, time: 0.959, data_time: 0.012, memory: 18982, decode.loss_ce: 1.4699, decode.acc_seg: 32.8518, loss: 1.4699
2023-02-19 20:17:28,127 - mmseg - INFO - Iter [1050/8000]	lr: 3.646e-05, eta: 1:58:30, time: 0.958, data_time: 0.012, memory: 18982, decode.loss_ce: 1.4355, decode.acc_seg: 33.6355, loss: 1.4355
2023-02-19 20:18:15,986 - mmseg - INFO - Iter [1100/8000]	lr: 3.792e-05, eta: 1:57:19, time: 0.957, data_time: 0.011, memory: 18982, decode.loss_ce: 1.3804, decode.acc_seg: 34.2551, loss: 1.3804
2023-02-19 20:19:03,832 - mmseg - INFO - Iter [1150/8000]	lr: 3.936e-05, eta: 1:56:09, time: 0.957, data_time: 0.011, memory: 18982, decode.loss_ce: 1.4414, decode.acc_seg: 35.9384, loss: 1.4414
2023-02-19 20:19:51,655 - mmseg - INFO - Iter [1200/8000]	lr: 4.077e-05, eta: 1:55:01, time: 0.956, data_time: 0.011, memory: 18982, decode.loss_ce: 1.3661, decode.acc_seg: 35.9579, loss: 1.3661
2023-02-19 20:20:39,472 - mmseg - INFO - Iter [1250/8000]	lr: 4.216e-05, eta: 1:53:54, time: 0.956, data_time: 0.011, memory: 18982, decode.loss_ce: 1.3208, decode.acc_seg: 35.4143, loss: 1.3208
2023-02-19 20:21:27,349 - mmseg - INFO - Iter [1300/8000]	lr: 4.352e-05, eta: 1:52:49, time: 0.958, data_time: 0.011, memory: 18982, decode.loss_ce: 1.3137, decode.acc_seg: 35.0638, loss: 1.3137
2023-02-19 20:22:15,282 - mmseg - INFO - Iter [1350/8000]	lr: 4.486e-05, eta: 1:51:46, time: 0.959, data_time: 0.011, memory: 18982, decode.loss_ce: 1.3745, decode.acc_seg: 33.0471, loss: 1.3745
2023-02-19 20:23:03,120 - mmseg - INFO - Iter [1400/8000]	lr: 4.617e-05, eta: 1:50:43, time: 0.957, data_time: 0.011, memory: 18982, decode.loss_ce: 1.3140, decode.acc_seg: 33.8718, loss: 1.3140
2023-02-19 20:23:51,017 - mmseg - INFO - Iter [1450/8000]	lr: 4.746e-05, eta: 1:49:42, time: 0.958, data_time: 0.012, memory: 18982, decode.loss_ce: 1.3520, decode.acc_seg: 36.4864, loss: 1.3520
2023-02-19 20:24:38,955 - mmseg - INFO - Iter [1500/8000]	lr: 4.872e-05, eta: 1:48:42, time: 0.959, data_time: 0.012, memory: 18982, decode.loss_ce: 1.3322, decode.acc_seg: 34.7643, loss: 1.3322
2023-02-19 20:25:26,888 - mmseg - INFO - Iter [1550/8000]	lr: 4.838e-05, eta: 1:47:42, time: 0.959, data_time: 0.011, memory: 18982, decode.loss_ce: 1.3724, decode.acc_seg: 34.1286, loss: 1.3724
2023-02-19 20:26:14,767 - mmseg - INFO - Iter [1600/8000]	lr: 4.801e-05, eta: 1:46:43, time: 0.958, data_time: 0.011, memory: 18982, decode.loss_ce: 1.2727, decode.acc_seg: 35.6296, loss: 1.2727
2023-02-19 20:27:02,605 - mmseg - INFO - Iter [1650/8000]	lr: 4.763e-05, eta: 1:45:45, time: 0.957, data_time: 0.011, memory: 18982, decode.loss_ce: 1.3336, decode.acc_seg: 37.0150, loss: 1.3336
2023-02-19 20:27:52,804 - mmseg - INFO - Iter [1700/8000]	lr: 4.726e-05, eta: 1:44:56, time: 1.004, data_time: 0.057, memory: 18982, decode.loss_ce: 1.2854, decode.acc_seg: 36.6125, loss: 1.2854
2023-02-19 20:28:40,763 - mmseg - INFO - Iter [1750/8000]	lr: 4.688e-05, eta: 1:43:59, time: 0.959, data_time: 0.011, memory: 18982, decode.loss_ce: 1.2791, decode.acc_seg: 36.3670, loss: 1.2791
2023-02-19 20:29:28,835 - mmseg - INFO - Iter [1800/8000]	lr: 4.651e-05, eta: 1:43:02, time: 0.961, data_time: 0.011, memory: 18982, decode.loss_ce: 1.2975, decode.acc_seg: 36.3139, loss: 1.2975
2023-02-19 20:30:16,876 - mmseg - INFO - Iter [1850/8000]	lr: 4.613e-05, eta: 1:42:06, time: 0.961, data_time: 0.012, memory: 18982, decode.loss_ce: 1.3765, decode.acc_seg: 36.9240, loss: 1.3765
2023-02-19 20:31:04,836 - mmseg - INFO - Iter [1900/8000]	lr: 4.576e-05, eta: 1:41:11, time: 0.959, data_time: 0.011, memory: 18982, decode.loss_ce: 1.3826, decode.acc_seg: 37.6099, loss: 1.3826
2023-02-19 20:31:52,756 - mmseg - INFO - Iter [1950/8000]	lr: 4.538e-05, eta: 1:40:15, time: 0.958, data_time: 0.011, memory: 18982, decode.loss_ce: 1.2326, decode.acc_seg: 36.0599, loss: 1.2326
2023-02-19 20:32:40,764 - mmseg - INFO - Exp name: maskclip_plus_r50_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 20:32:40,764 - mmseg - INFO - Iter [2000/8000]	lr: 4.501e-05, eta: 1:39:20, time: 0.960, data_time: 0.011, memory: 18982, decode.loss_ce: 1.3254, decode.acc_seg: 38.9180, loss: 1.3254
2023-02-19 20:33:28,779 - mmseg - INFO - Iter [2050/8000]	lr: 4.463e-05, eta: 1:38:26, time: 0.960, data_time: 0.011, memory: 18982, decode.loss_ce: 1.2707, decode.acc_seg: 36.3248, loss: 1.2707
2023-02-19 20:34:16,872 - mmseg - INFO - Iter [2100/8000]	lr: 4.426e-05, eta: 1:37:32, time: 0.962, data_time: 0.012, memory: 18982, decode.loss_ce: 1.2269, decode.acc_seg: 35.4797, loss: 1.2269
2023-02-19 20:35:04,975 - mmseg - INFO - Iter [2150/8000]	lr: 4.388e-05, eta: 1:36:38, time: 0.962, data_time: 0.012, memory: 18982, decode.loss_ce: 1.2329, decode.acc_seg: 37.8138, loss: 1.2329
2023-02-19 20:35:53,029 - mmseg - INFO - Iter [2200/8000]	lr: 4.351e-05, eta: 1:35:45, time: 0.961, data_time: 0.011, memory: 18982, decode.loss_ce: 1.2669, decode.acc_seg: 36.7429, loss: 1.2669
2023-02-19 20:36:41,054 - mmseg - INFO - Iter [2250/8000]	lr: 4.313e-05, eta: 1:34:51, time: 0.960, data_time: 0.011, memory: 18982, decode.loss_ce: 1.2620, decode.acc_seg: 36.6394, loss: 1.2620
2023-02-19 20:37:29,269 - mmseg - INFO - Iter [2300/8000]	lr: 4.276e-05, eta: 1:33:59, time: 0.964, data_time: 0.012, memory: 18982, decode.loss_ce: 1.2503, decode.acc_seg: 34.9860, loss: 1.2503
2023-02-19 20:38:18,045 - mmseg - INFO - Iter [2350/8000]	lr: 4.238e-05, eta: 1:33:08, time: 0.975, data_time: 0.014, memory: 18982, decode.loss_ce: 1.2860, decode.acc_seg: 37.8948, loss: 1.2860
2023-02-19 20:39:06,602 - mmseg - INFO - Iter [2400/8000]	lr: 4.201e-05, eta: 1:32:16, time: 0.971, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1945, decode.acc_seg: 35.6470, loss: 1.1945
2023-02-19 20:39:55,131 - mmseg - INFO - Iter [2450/8000]	lr: 4.163e-05, eta: 1:31:24, time: 0.971, data_time: 0.013, memory: 18982, decode.loss_ce: 1.2271, decode.acc_seg: 37.0550, loss: 1.2271
2023-02-19 20:40:46,026 - mmseg - INFO - Iter [2500/8000]	lr: 4.126e-05, eta: 1:30:38, time: 1.018, data_time: 0.060, memory: 18982, decode.loss_ce: 1.1743, decode.acc_seg: 36.7934, loss: 1.1743
2023-02-19 20:41:34,433 - mmseg - INFO - Iter [2550/8000]	lr: 4.088e-05, eta: 1:29:47, time: 0.968, data_time: 0.013, memory: 18982, decode.loss_ce: 1.2180, decode.acc_seg: 36.3041, loss: 1.2180
2023-02-19 20:42:22,843 - mmseg - INFO - Iter [2600/8000]	lr: 4.051e-05, eta: 1:28:55, time: 0.968, data_time: 0.013, memory: 18982, decode.loss_ce: 1.2561, decode.acc_seg: 37.0694, loss: 1.2561
2023-02-19 20:43:11,273 - mmseg - INFO - Iter [2650/8000]	lr: 4.013e-05, eta: 1:28:04, time: 0.969, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1962, decode.acc_seg: 36.5288, loss: 1.1962
2023-02-19 20:43:59,741 - mmseg - INFO - Iter [2700/8000]	lr: 3.976e-05, eta: 1:27:13, time: 0.969, data_time: 0.014, memory: 18982, decode.loss_ce: 1.2010, decode.acc_seg: 37.3510, loss: 1.2010
2023-02-19 20:44:48,204 - mmseg - INFO - Iter [2750/8000]	lr: 3.938e-05, eta: 1:26:21, time: 0.969, data_time: 0.014, memory: 18982, decode.loss_ce: 1.2616, decode.acc_seg: 37.7875, loss: 1.2616
2023-02-19 20:45:36,699 - mmseg - INFO - Iter [2800/8000]	lr: 3.901e-05, eta: 1:25:31, time: 0.970, data_time: 0.014, memory: 18982, decode.loss_ce: 1.1307, decode.acc_seg: 38.6105, loss: 1.1307
2023-02-19 20:46:25,190 - mmseg - INFO - Iter [2850/8000]	lr: 3.863e-05, eta: 1:24:40, time: 0.970, data_time: 0.014, memory: 18982, decode.loss_ce: 1.2170, decode.acc_seg: 39.2057, loss: 1.2170
2023-02-19 20:47:13,682 - mmseg - INFO - Iter [2900/8000]	lr: 3.826e-05, eta: 1:23:49, time: 0.970, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1501, decode.acc_seg: 36.3372, loss: 1.1501
2023-02-19 20:48:02,150 - mmseg - INFO - Iter [2950/8000]	lr: 3.788e-05, eta: 1:22:58, time: 0.969, data_time: 0.014, memory: 18982, decode.loss_ce: 1.1822, decode.acc_seg: 38.3482, loss: 1.1822
2023-02-19 20:48:50,635 - mmseg - INFO - Exp name: maskclip_plus_r50_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 20:48:50,635 - mmseg - INFO - Iter [3000/8000]	lr: 3.751e-05, eta: 1:22:08, time: 0.970, data_time: 0.014, memory: 18982, decode.loss_ce: 1.1844, decode.acc_seg: 36.8313, loss: 1.1844
2023-02-19 20:49:39,141 - mmseg - INFO - Iter [3050/8000]	lr: 3.713e-05, eta: 1:21:17, time: 0.970, data_time: 0.014, memory: 18982, decode.loss_ce: 1.2919, decode.acc_seg: 38.4933, loss: 1.2919
2023-02-19 20:50:27,574 - mmseg - INFO - Iter [3100/8000]	lr: 3.676e-05, eta: 1:20:26, time: 0.969, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1621, decode.acc_seg: 36.0784, loss: 1.1621
2023-02-19 20:51:16,020 - mmseg - INFO - Iter [3150/8000]	lr: 3.638e-05, eta: 1:19:36, time: 0.969, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1156, decode.acc_seg: 37.0955, loss: 1.1156
2023-02-19 20:52:04,449 - mmseg - INFO - Iter [3200/8000]	lr: 3.601e-05, eta: 1:18:45, time: 0.969, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1831, decode.acc_seg: 38.7652, loss: 1.1831
2023-02-19 20:52:52,857 - mmseg - INFO - Iter [3250/8000]	lr: 3.563e-05, eta: 1:17:55, time: 0.968, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1306, decode.acc_seg: 37.2350, loss: 1.1306
2023-02-19 20:53:41,281 - mmseg - INFO - Iter [3300/8000]	lr: 3.526e-05, eta: 1:17:05, time: 0.968, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1121, decode.acc_seg: 36.3325, loss: 1.1121
2023-02-19 20:54:32,013 - mmseg - INFO - Iter [3350/8000]	lr: 3.488e-05, eta: 1:16:18, time: 1.015, data_time: 0.060, memory: 18982, decode.loss_ce: 1.1507, decode.acc_seg: 38.6481, loss: 1.1507
2023-02-19 20:55:20,448 - mmseg - INFO - Iter [3400/8000]	lr: 3.451e-05, eta: 1:15:27, time: 0.969, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1064, decode.acc_seg: 38.4041, loss: 1.1064
2023-02-19 20:56:08,883 - mmseg - INFO - Iter [3450/8000]	lr: 3.413e-05, eta: 1:14:37, time: 0.969, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1321, decode.acc_seg: 37.4409, loss: 1.1321
2023-02-19 20:56:57,361 - mmseg - INFO - Iter [3500/8000]	lr: 3.376e-05, eta: 1:13:47, time: 0.970, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1917, decode.acc_seg: 40.3583, loss: 1.1917
2023-02-19 20:57:45,822 - mmseg - INFO - Iter [3550/8000]	lr: 3.338e-05, eta: 1:12:57, time: 0.969, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1293, decode.acc_seg: 37.0081, loss: 1.1293
2023-02-19 20:58:34,282 - mmseg - INFO - Iter [3600/8000]	lr: 3.301e-05, eta: 1:12:07, time: 0.969, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1376, decode.acc_seg: 38.9100, loss: 1.1376
2023-02-19 20:59:22,779 - mmseg - INFO - Iter [3650/8000]	lr: 3.263e-05, eta: 1:11:17, time: 0.970, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1154, decode.acc_seg: 37.6519, loss: 1.1154
2023-02-19 21:00:11,204 - mmseg - INFO - Iter [3700/8000]	lr: 3.226e-05, eta: 1:10:27, time: 0.969, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1018, decode.acc_seg: 37.6783, loss: 1.1018
2023-02-19 21:00:59,647 - mmseg - INFO - Iter [3750/8000]	lr: 3.188e-05, eta: 1:09:37, time: 0.969, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1252, decode.acc_seg: 39.3868, loss: 1.1252
2023-02-19 21:01:48,111 - mmseg - INFO - Iter [3800/8000]	lr: 3.151e-05, eta: 1:08:47, time: 0.969, data_time: 0.014, memory: 18982, decode.loss_ce: 1.1881, decode.acc_seg: 37.4006, loss: 1.1881
2023-02-19 21:02:36,523 - mmseg - INFO - Iter [3850/8000]	lr: 3.113e-05, eta: 1:07:57, time: 0.968, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1114, decode.acc_seg: 37.3819, loss: 1.1114
2023-02-19 21:03:24,946 - mmseg - INFO - Iter [3900/8000]	lr: 3.076e-05, eta: 1:07:07, time: 0.968, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1898, decode.acc_seg: 38.6078, loss: 1.1898
2023-02-19 21:04:13,346 - mmseg - INFO - Iter [3950/8000]	lr: 3.038e-05, eta: 1:06:17, time: 0.968, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0834, decode.acc_seg: 37.5424, loss: 1.0834
2023-02-19 21:05:01,760 - mmseg - INFO - Saving checkpoint at 4000 iterations
2023-02-19 21:05:04,081 - mmseg - INFO - Exp name: maskclip_plus_r50_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 21:05:04,081 - mmseg - INFO - Iter [4000/8000]	lr: 3.001e-05, eta: 1:05:30, time: 1.015, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0234, decode.acc_seg: 37.4154, loss: 1.0234
2023-02-19 21:28:58,119 - mmseg - INFO - per class results:
2023-02-19 21:28:58,123 - mmseg - INFO - 
+-------------+-------+-------+-------+
|    Class    |  IoU  |  Acc  |  Prec |
+-------------+-------+-------+-------+
|  aeroplane  | 41.69 |  84.9 | 45.02 |
|     bag     | 14.28 | 35.56 | 19.27 |
|     bed     |  4.49 | 22.28 |  5.33 |
|  bedclothes | 11.84 | 33.31 | 15.51 |
|    bench    |  3.77 | 26.13 |  4.22 |
|   bicycle   | 45.58 | 84.11 | 49.87 |
|     bird    | 43.67 | 74.58 | 51.31 |
|     boat    | 17.23 | 63.26 | 19.14 |
|     book    |  2.79 |  2.85 | 59.24 |
|    bottle   | 47.24 | 68.65 | 60.23 |
|   building  | 13.61 | 14.05 | 81.27 |
|     bus     | 65.74 | 79.19 | 79.47 |
|   cabinet   | 17.71 | 27.27 | 33.57 |
|     car     | 59.13 | 75.22 | 73.43 |
|     cat     | 61.51 | 69.49 | 84.27 |
|   ceiling   | 28.43 | 46.47 | 42.28 |
|    chair    | 23.85 | 29.89 | 54.13 |
|    cloth    |  3.46 |  7.8  |  5.86 |
|   computer  |  1.88 |  83.0 |  1.89 |
|     cow     | 20.71 | 45.87 | 27.42 |
|     cup     | 13.06 | 21.22 | 25.35 |
|   curtain   | 15.14 | 55.61 | 17.22 |
|     dog     | 53.97 | 64.36 | 76.97 |
|     door    |  8.28 | 49.66 |  9.04 |
|    fence    | 13.79 |  55.8 | 15.48 |
|    floor    |  31.6 | 50.74 | 45.58 |
|    flower   | 17.49 | 34.75 | 26.05 |
|     food    | 15.52 | 72.06 | 16.51 |
|    grass    | 48.77 |  54.5 | 82.25 |
|    ground   |  5.65 |  5.96 | 52.17 |
|    horse    | 25.29 | 86.57 | 26.32 |
|   keyboard  | 39.72 | 51.23 | 63.86 |
|    light    |  6.31 | 19.36 |  8.56 |
|  motorbike  | 47.55 | 59.26 | 70.64 |
|   mountain  | 16.52 |  40.1 | 21.94 |
|    mouse    |  0.01 | 10.02 |  0.01 |
|    person   | 23.59 | 27.35 | 63.18 |
|    plate    |  2.45 |  3.79 |  6.45 |
|   platform  |  5.58 | 23.01 |  6.86 |
| pottedplant | 28.92 | 74.42 | 32.11 |
|     road    | 21.87 | 26.42 | 55.94 |
|     rock    | 16.79 |  47.3 | 20.66 |
|    sheep    |  8.75 | 61.17 |  9.27 |
|   shelves   |  8.3  | 35.21 |  9.8  |
|   sidewalk  |  9.26 | 52.53 |  10.1 |
|     sign    | 22.15 |  45.0 | 30.37 |
|     sky     | 54.45 | 56.25 | 94.46 |
|     snow    | 29.58 | 66.68 | 34.71 |
|     sofa    | 28.97 | 57.66 |  36.8 |
|    table    | 22.25 | 33.04 | 40.52 |
|    track    |  0.0  |  0.0  |  0.0  |
|    train    | 46.81 | 78.03 | 53.91 |
|     tree    | 32.97 | 34.27 | 89.67 |
|    truck    |  8.5  | 34.21 | 10.17 |
|  tvmonitor  | 24.04 | 30.54 | 53.02 |
|     wall    | 14.93 |  16.4 | 62.39 |
|    water    | 32.53 | 36.92 | 73.21 |
|    window   |  13.1 | 30.81 | 18.57 |
|     wood    |  9.67 | 44.28 | 11.02 |
+-------------+-------+-------+-------+
2023-02-19 21:28:58,123 - mmseg - INFO - Summary:
2023-02-19 21:28:58,123 - mmseg - INFO - 
+-------+-------+-------+-------+
|  aAcc |  mIoU |  mAcc | mPrec |
+-------+-------+-------+-------+
| 41.45 | 22.93 | 44.41 | 37.18 |
+-------+-------+-------+-------+
2023-02-19 21:28:58,136 - mmseg - INFO - Exp name: maskclip_plus_r50_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 21:28:58,137 - mmseg - INFO - Iter(val) [5104]	aAcc: 0.4145, mIoU: 0.2293, mAcc: 0.4441, mPrec: 0.3718, IoU.aeroplane: 0.4169, IoU.bag: 0.1428, IoU.bed: 0.0449, IoU.bedclothes: 0.1184, IoU.bench: 0.0377, IoU.bicycle: 0.4558, IoU.bird: 0.4367, IoU.boat: 0.1723, IoU.book: 0.0279, IoU.bottle: 0.4724, IoU.building: 0.1361, IoU.bus: 0.6574, IoU.cabinet: 0.1771, IoU.car: 0.5913, IoU.cat: 0.6151, IoU.ceiling: 0.2843, IoU.chair: 0.2385, IoU.cloth: 0.0346, IoU.computer: 0.0188, IoU.cow: 0.2071, IoU.cup: 0.1306, IoU.curtain: 0.1514, IoU.dog: 0.5397, IoU.door: 0.0828, IoU.fence: 0.1379, IoU.floor: 0.3160, IoU.flower: 0.1749, IoU.food: 0.1552, IoU.grass: 0.4877, IoU.ground: 0.0565, IoU.horse: 0.2529, IoU.keyboard: 0.3972, IoU.light: 0.0631, IoU.motorbike: 0.4755, IoU.mountain: 0.1652, IoU.mouse: 0.0001, IoU.person: 0.2359, IoU.plate: 0.0245, IoU.platform: 0.0558, IoU.pottedplant: 0.2892, IoU.road: 0.2187, IoU.rock: 0.1679, IoU.sheep: 0.0875, IoU.shelves: 0.0830, IoU.sidewalk: 0.0926, IoU.sign: 0.2215, IoU.sky: 0.5445, IoU.snow: 0.2958, IoU.sofa: 0.2897, IoU.table: 0.2225, IoU.track: 0.0000, IoU.train: 0.4681, IoU.tree: 0.3297, IoU.truck: 0.0850, IoU.tvmonitor: 0.2404, IoU.wall: 0.1493, IoU.water: 0.3253, IoU.window: 0.1310, IoU.wood: 0.0967, Acc.aeroplane: 0.8490, Acc.bag: 0.3556, Acc.bed: 0.2228, Acc.bedclothes: 0.3331, Acc.bench: 0.2613, Acc.bicycle: 0.8411, Acc.bird: 0.7458, Acc.boat: 0.6326, Acc.book: 0.0285, Acc.bottle: 0.6865, Acc.building: 0.1405, Acc.bus: 0.7919, Acc.cabinet: 0.2727, Acc.car: 0.7522, Acc.cat: 0.6949, Acc.ceiling: 0.4647, Acc.chair: 0.2989, Acc.cloth: 0.0780, Acc.computer: 0.8300, Acc.cow: 0.4587, Acc.cup: 0.2122, Acc.curtain: 0.5561, Acc.dog: 0.6436, Acc.door: 0.4966, Acc.fence: 0.5580, Acc.floor: 0.5074, Acc.flower: 0.3475, Acc.food: 0.7206, Acc.grass: 0.5450, Acc.ground: 0.0596, Acc.horse: 0.8657, Acc.keyboard: 0.5123, Acc.light: 0.1936, Acc.motorbike: 0.5926, Acc.mountain: 0.4010, Acc.mouse: 0.1002, Acc.person: 0.2735, Acc.plate: 0.0379, Acc.platform: 0.2301, Acc.pottedplant: 0.7442, Acc.road: 0.2642, Acc.rock: 0.4730, Acc.sheep: 0.6117, Acc.shelves: 0.3521, Acc.sidewalk: 0.5253, Acc.sign: 0.4500, Acc.sky: 0.5625, Acc.snow: 0.6668, Acc.sofa: 0.5766, Acc.table: 0.3304, Acc.track: 0.0000, Acc.train: 0.7803, Acc.tree: 0.3427, Acc.truck: 0.3421, Acc.tvmonitor: 0.3054, Acc.wall: 0.1640, Acc.water: 0.3692, Acc.window: 0.3081, Acc.wood: 0.4428, Prec.aeroplane: 0.4502, Prec.bag: 0.1927, Prec.bed: 0.0533, Prec.bedclothes: 0.1551, Prec.bench: 0.0422, Prec.bicycle: 0.4987, Prec.bird: 0.5131, Prec.boat: 0.1914, Prec.book: 0.5924, Prec.bottle: 0.6023, Prec.building: 0.8127, Prec.bus: 0.7947, Prec.cabinet: 0.3357, Prec.car: 0.7343, Prec.cat: 0.8427, Prec.ceiling: 0.4228, Prec.chair: 0.5413, Prec.cloth: 0.0586, Prec.computer: 0.0189, Prec.cow: 0.2742, Prec.cup: 0.2535, Prec.curtain: 0.1722, Prec.dog: 0.7697, Prec.door: 0.0904, Prec.fence: 0.1548, Prec.floor: 0.4558, Prec.flower: 0.2605, Prec.food: 0.1651, Prec.grass: 0.8225, Prec.ground: 0.5217, Prec.horse: 0.2632, Prec.keyboard: 0.6386, Prec.light: 0.0856, Prec.motorbike: 0.7064, Prec.mountain: 0.2194, Prec.mouse: 0.0001, Prec.person: 0.6318, Prec.plate: 0.0645, Prec.platform: 0.0686, Prec.pottedplant: 0.3211, Prec.road: 0.5594, Prec.rock: 0.2066, Prec.sheep: 0.0927, Prec.shelves: 0.0980, Prec.sidewalk: 0.1010, Prec.sign: 0.3037, Prec.sky: 0.9446, Prec.snow: 0.3471, Prec.sofa: 0.3680, Prec.table: 0.4052, Prec.track: 0.0000, Prec.train: 0.5391, Prec.tree: 0.8967, Prec.truck: 0.1017, Prec.tvmonitor: 0.5302, Prec.wall: 0.6239, Prec.water: 0.7321, Prec.window: 0.1857, Prec.wood: 0.1102
2023-02-19 21:29:46,413 - mmseg - INFO - Iter [4050/8000]	lr: 2.963e-05, eta: 1:27:58, time: 29.647, data_time: 28.694, memory: 18982, decode.loss_ce: 1.1159, decode.acc_seg: 39.5759, loss: 1.1159
2023-02-19 21:30:34,822 - mmseg - INFO - Iter [4100/8000]	lr: 2.926e-05, eta: 1:26:34, time: 0.968, data_time: 0.014, memory: 18982, decode.loss_ce: 1.1104, decode.acc_seg: 37.1782, loss: 1.1104
2023-02-19 21:31:23,285 - mmseg - INFO - Iter [4150/8000]	lr: 2.888e-05, eta: 1:25:11, time: 0.969, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0815, decode.acc_seg: 38.2598, loss: 1.0815
2023-02-19 21:32:13,929 - mmseg - INFO - Iter [4200/8000]	lr: 2.851e-05, eta: 1:23:50, time: 1.013, data_time: 0.059, memory: 18982, decode.loss_ce: 1.1343, decode.acc_seg: 39.9137, loss: 1.1343
2023-02-19 21:33:02,272 - mmseg - INFO - Iter [4250/8000]	lr: 2.813e-05, eta: 1:22:28, time: 0.967, data_time: 0.014, memory: 18982, decode.loss_ce: 1.1368, decode.acc_seg: 39.4455, loss: 1.1368
2023-02-19 21:33:50,818 - mmseg - INFO - Iter [4300/8000]	lr: 2.776e-05, eta: 1:21:07, time: 0.971, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0535, decode.acc_seg: 37.1730, loss: 1.0535
2023-02-19 21:34:39,250 - mmseg - INFO - Iter [4350/8000]	lr: 2.738e-05, eta: 1:19:47, time: 0.969, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0288, decode.acc_seg: 39.5543, loss: 1.0288
2023-02-19 21:35:27,514 - mmseg - INFO - Iter [4400/8000]	lr: 2.701e-05, eta: 1:18:27, time: 0.965, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1427, decode.acc_seg: 38.9722, loss: 1.1427
2023-02-19 21:36:15,688 - mmseg - INFO - Iter [4450/8000]	lr: 2.663e-05, eta: 1:17:08, time: 0.963, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1658, decode.acc_seg: 39.0432, loss: 1.1658
2023-02-19 21:37:03,985 - mmseg - INFO - Iter [4500/8000]	lr: 2.626e-05, eta: 1:15:50, time: 0.966, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0430, decode.acc_seg: 38.6477, loss: 1.0430
2023-02-19 21:37:52,637 - mmseg - INFO - Iter [4550/8000]	lr: 2.588e-05, eta: 1:14:32, time: 0.973, data_time: 0.015, memory: 18982, decode.loss_ce: 1.1490, decode.acc_seg: 39.4904, loss: 1.1490
2023-02-19 21:38:40,783 - mmseg - INFO - Iter [4600/8000]	lr: 2.551e-05, eta: 1:13:15, time: 0.963, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1061, decode.acc_seg: 39.7999, loss: 1.1061
2023-02-19 21:39:28,929 - mmseg - INFO - Iter [4650/8000]	lr: 2.513e-05, eta: 1:11:59, time: 0.963, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1189, decode.acc_seg: 38.0135, loss: 1.1189
2023-02-19 21:40:17,120 - mmseg - INFO - Iter [4700/8000]	lr: 2.476e-05, eta: 1:10:43, time: 0.964, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1067, decode.acc_seg: 40.1284, loss: 1.1067
2023-02-19 21:41:05,664 - mmseg - INFO - Iter [4750/8000]	lr: 2.438e-05, eta: 1:09:28, time: 0.971, data_time: 0.015, memory: 18982, decode.loss_ce: 1.0790, decode.acc_seg: 38.5505, loss: 1.0790
2023-02-19 21:41:53,897 - mmseg - INFO - Iter [4800/8000]	lr: 2.401e-05, eta: 1:08:13, time: 0.965, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0796, decode.acc_seg: 38.2006, loss: 1.0796
2023-02-19 21:42:41,996 - mmseg - INFO - Iter [4850/8000]	lr: 2.363e-05, eta: 1:06:59, time: 0.962, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1466, decode.acc_seg: 39.9693, loss: 1.1466
2023-02-19 21:43:30,072 - mmseg - INFO - Iter [4900/8000]	lr: 2.326e-05, eta: 1:05:45, time: 0.961, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1088, decode.acc_seg: 39.2344, loss: 1.1088
2023-02-19 21:44:18,134 - mmseg - INFO - Iter [4950/8000]	lr: 2.288e-05, eta: 1:04:32, time: 0.961, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0989, decode.acc_seg: 39.8096, loss: 1.0989
2023-02-19 21:45:08,553 - mmseg - INFO - Exp name: maskclip_plus_r50_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 21:45:08,554 - mmseg - INFO - Iter [5000/8000]	lr: 2.251e-05, eta: 1:03:20, time: 1.008, data_time: 0.060, memory: 18982, decode.loss_ce: 1.1228, decode.acc_seg: 40.5706, loss: 1.1228
2023-02-19 21:45:56,804 - mmseg - INFO - Iter [5050/8000]	lr: 2.213e-05, eta: 1:02:08, time: 0.965, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1414, decode.acc_seg: 39.3393, loss: 1.1414
2023-02-19 21:46:44,886 - mmseg - INFO - Iter [5100/8000]	lr: 2.176e-05, eta: 1:00:56, time: 0.962, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1191, decode.acc_seg: 39.6767, loss: 1.1191
2023-02-19 21:47:33,115 - mmseg - INFO - Iter [5150/8000]	lr: 2.138e-05, eta: 0:59:45, time: 0.965, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0932, decode.acc_seg: 39.3565, loss: 1.0932
2023-02-19 21:48:21,282 - mmseg - INFO - Iter [5200/8000]	lr: 2.101e-05, eta: 0:58:34, time: 0.963, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1771, decode.acc_seg: 41.6489, loss: 1.1771
2023-02-19 21:49:09,471 - mmseg - INFO - Iter [5250/8000]	lr: 2.063e-05, eta: 0:57:24, time: 0.964, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0504, decode.acc_seg: 38.4993, loss: 1.0504
2023-02-19 21:49:57,667 - mmseg - INFO - Iter [5300/8000]	lr: 2.026e-05, eta: 0:56:14, time: 0.964, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0549, decode.acc_seg: 39.7377, loss: 1.0549
2023-02-19 21:50:45,792 - mmseg - INFO - Iter [5350/8000]	lr: 1.988e-05, eta: 0:55:04, time: 0.963, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0526, decode.acc_seg: 40.1482, loss: 1.0526
2023-02-19 21:51:33,921 - mmseg - INFO - Iter [5400/8000]	lr: 1.951e-05, eta: 0:53:55, time: 0.963, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0672, decode.acc_seg: 40.2318, loss: 1.0672
2023-02-19 21:52:22,287 - mmseg - INFO - Iter [5450/8000]	lr: 1.913e-05, eta: 0:52:46, time: 0.967, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0918, decode.acc_seg: 41.5459, loss: 1.0918
2023-02-19 21:53:10,468 - mmseg - INFO - Iter [5500/8000]	lr: 1.876e-05, eta: 0:51:38, time: 0.964, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1341, decode.acc_seg: 40.9321, loss: 1.1341
2023-02-19 21:53:58,665 - mmseg - INFO - Iter [5550/8000]	lr: 1.838e-05, eta: 0:50:30, time: 0.964, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0962, decode.acc_seg: 39.3529, loss: 1.0962
2023-02-19 21:54:46,828 - mmseg - INFO - Iter [5600/8000]	lr: 1.801e-05, eta: 0:49:22, time: 0.963, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0942, decode.acc_seg: 39.8940, loss: 1.0942
2023-02-19 21:55:35,121 - mmseg - INFO - Iter [5650/8000]	lr: 1.763e-05, eta: 0:48:15, time: 0.966, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0443, decode.acc_seg: 38.4683, loss: 1.0443
2023-02-19 21:56:23,273 - mmseg - INFO - Iter [5700/8000]	lr: 1.726e-05, eta: 0:47:08, time: 0.963, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0541, decode.acc_seg: 39.4346, loss: 1.0541
2023-02-19 21:57:11,514 - mmseg - INFO - Iter [5750/8000]	lr: 1.688e-05, eta: 0:46:01, time: 0.965, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0942, decode.acc_seg: 41.0646, loss: 1.0942
2023-02-19 21:57:59,713 - mmseg - INFO - Iter [5800/8000]	lr: 1.651e-05, eta: 0:44:55, time: 0.964, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0933, decode.acc_seg: 38.4770, loss: 1.0933
2023-02-19 21:58:50,200 - mmseg - INFO - Iter [5850/8000]	lr: 1.613e-05, eta: 0:43:50, time: 1.010, data_time: 0.059, memory: 18982, decode.loss_ce: 1.0739, decode.acc_seg: 39.3974, loss: 1.0739
2023-02-19 21:59:38,544 - mmseg - INFO - Iter [5900/8000]	lr: 1.576e-05, eta: 0:42:44, time: 0.967, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0717, decode.acc_seg: 41.2640, loss: 1.0717
2023-02-19 22:00:26,659 - mmseg - INFO - Iter [5950/8000]	lr: 1.538e-05, eta: 0:41:38, time: 0.962, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0659, decode.acc_seg: 40.9557, loss: 1.0659
2023-02-19 22:01:14,825 - mmseg - INFO - Exp name: maskclip_plus_r50_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 22:01:14,825 - mmseg - INFO - Iter [6000/8000]	lr: 1.501e-05, eta: 0:40:33, time: 0.963, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0791, decode.acc_seg: 39.7855, loss: 1.0791
2023-02-19 22:02:02,928 - mmseg - INFO - Iter [6050/8000]	lr: 1.463e-05, eta: 0:39:28, time: 0.962, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0879, decode.acc_seg: 41.3009, loss: 1.0879
2023-02-19 22:02:51,188 - mmseg - INFO - Iter [6100/8000]	lr: 1.426e-05, eta: 0:38:24, time: 0.965, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0734, decode.acc_seg: 40.6596, loss: 1.0734
2023-02-19 22:03:39,451 - mmseg - INFO - Iter [6150/8000]	lr: 1.388e-05, eta: 0:37:19, time: 0.965, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0588, decode.acc_seg: 40.2234, loss: 1.0588
2023-02-19 22:04:27,657 - mmseg - INFO - Iter [6200/8000]	lr: 1.351e-05, eta: 0:36:15, time: 0.964, data_time: 0.014, memory: 18982, decode.loss_ce: 0.9925, decode.acc_seg: 41.0032, loss: 0.9925
2023-02-19 22:05:15,813 - mmseg - INFO - Iter [6250/8000]	lr: 1.313e-05, eta: 0:35:11, time: 0.963, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0679, decode.acc_seg: 38.5171, loss: 1.0679
2023-02-19 22:06:04,011 - mmseg - INFO - Iter [6300/8000]	lr: 1.276e-05, eta: 0:34:08, time: 0.964, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0279, decode.acc_seg: 40.3345, loss: 1.0279
2023-02-19 22:06:52,178 - mmseg - INFO - Iter [6350/8000]	lr: 1.238e-05, eta: 0:33:04, time: 0.963, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0858, decode.acc_seg: 41.6066, loss: 1.0858
2023-02-19 22:07:40,463 - mmseg - INFO - Iter [6400/8000]	lr: 1.201e-05, eta: 0:32:01, time: 0.966, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0972, decode.acc_seg: 40.1526, loss: 1.0972
2023-02-19 22:08:28,596 - mmseg - INFO - Iter [6450/8000]	lr: 1.163e-05, eta: 0:30:58, time: 0.963, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0454, decode.acc_seg: 39.9533, loss: 1.0454
2023-02-19 22:09:16,813 - mmseg - INFO - Iter [6500/8000]	lr: 1.126e-05, eta: 0:29:56, time: 0.964, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0496, decode.acc_seg: 38.7031, loss: 1.0496
2023-02-19 22:10:05,085 - mmseg - INFO - Iter [6550/8000]	lr: 1.088e-05, eta: 0:28:53, time: 0.965, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0772, decode.acc_seg: 39.1164, loss: 1.0772
2023-02-19 22:10:53,336 - mmseg - INFO - Iter [6600/8000]	lr: 1.051e-05, eta: 0:27:51, time: 0.965, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0647, decode.acc_seg: 39.9521, loss: 1.0647
2023-02-19 22:11:41,593 - mmseg - INFO - Iter [6650/8000]	lr: 1.013e-05, eta: 0:26:49, time: 0.965, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0730, decode.acc_seg: 40.9920, loss: 1.0730
2023-02-19 22:12:31,945 - mmseg - INFO - Iter [6700/8000]	lr: 9.758e-06, eta: 0:25:47, time: 1.007, data_time: 0.058, memory: 18982, decode.loss_ce: 1.0154, decode.acc_seg: 40.1326, loss: 1.0154
2023-02-19 22:13:20,006 - mmseg - INFO - Iter [6750/8000]	lr: 9.383e-06, eta: 0:24:46, time: 0.961, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0508, decode.acc_seg: 39.4789, loss: 1.0508
2023-02-19 22:14:07,966 - mmseg - INFO - Iter [6800/8000]	lr: 9.007e-06, eta: 0:23:44, time: 0.959, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0591, decode.acc_seg: 39.6861, loss: 1.0591
2023-02-19 22:14:55,900 - mmseg - INFO - Iter [6850/8000]	lr: 8.632e-06, eta: 0:22:43, time: 0.959, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0374, decode.acc_seg: 40.9412, loss: 1.0374
2023-02-19 22:15:43,827 - mmseg - INFO - Iter [6900/8000]	lr: 8.258e-06, eta: 0:21:42, time: 0.959, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0771, decode.acc_seg: 39.4842, loss: 1.0771
2023-02-19 22:16:31,808 - mmseg - INFO - Iter [6950/8000]	lr: 7.883e-06, eta: 0:20:41, time: 0.960, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0258, decode.acc_seg: 39.6496, loss: 1.0258
2023-02-19 22:17:20,026 - mmseg - INFO - Exp name: maskclip_plus_r50_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 22:17:20,026 - mmseg - INFO - Iter [7000/8000]	lr: 7.508e-06, eta: 0:19:40, time: 0.964, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0179, decode.acc_seg: 41.4278, loss: 1.0179
2023-02-19 22:18:08,267 - mmseg - INFO - Iter [7050/8000]	lr: 7.132e-06, eta: 0:18:40, time: 0.965, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0761, decode.acc_seg: 42.5075, loss: 1.0761
2023-02-19 22:18:56,597 - mmseg - INFO - Iter [7100/8000]	lr: 6.757e-06, eta: 0:17:40, time: 0.967, data_time: 0.014, memory: 18982, decode.loss_ce: 1.1194, decode.acc_seg: 40.2144, loss: 1.1194
2023-02-19 22:19:44,905 - mmseg - INFO - Iter [7150/8000]	lr: 6.383e-06, eta: 0:16:39, time: 0.966, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0678, decode.acc_seg: 40.3001, loss: 1.0678
2023-02-19 22:20:33,229 - mmseg - INFO - Iter [7200/8000]	lr: 6.008e-06, eta: 0:15:39, time: 0.966, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0056, decode.acc_seg: 39.8865, loss: 1.0056
2023-02-19 22:21:21,401 - mmseg - INFO - Iter [7250/8000]	lr: 5.633e-06, eta: 0:14:40, time: 0.963, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0981, decode.acc_seg: 39.7507, loss: 1.0981
2023-02-19 22:22:09,609 - mmseg - INFO - Iter [7300/8000]	lr: 5.257e-06, eta: 0:13:40, time: 0.964, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1049, decode.acc_seg: 41.6217, loss: 1.1049
2023-02-19 22:22:57,761 - mmseg - INFO - Iter [7350/8000]	lr: 4.882e-06, eta: 0:12:40, time: 0.963, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0319, decode.acc_seg: 41.2532, loss: 1.0319
2023-02-19 22:23:45,891 - mmseg - INFO - Iter [7400/8000]	lr: 4.507e-06, eta: 0:11:41, time: 0.963, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0471, decode.acc_seg: 38.0979, loss: 1.0471
2023-02-19 22:24:34,016 - mmseg - INFO - Iter [7450/8000]	lr: 4.133e-06, eta: 0:10:42, time: 0.962, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0398, decode.acc_seg: 41.7777, loss: 1.0398
2023-02-19 22:25:24,401 - mmseg - INFO - Iter [7500/8000]	lr: 3.758e-06, eta: 0:09:43, time: 1.008, data_time: 0.058, memory: 18982, decode.loss_ce: 1.0227, decode.acc_seg: 41.1068, loss: 1.0227
2023-02-19 22:26:12,478 - mmseg - INFO - Iter [7550/8000]	lr: 3.382e-06, eta: 0:08:44, time: 0.962, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0183, decode.acc_seg: 40.2053, loss: 1.0183
2023-02-19 22:27:00,562 - mmseg - INFO - Iter [7600/8000]	lr: 3.007e-06, eta: 0:07:45, time: 0.962, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0687, decode.acc_seg: 40.9819, loss: 1.0687
2023-02-19 22:27:48,682 - mmseg - INFO - Iter [7650/8000]	lr: 2.632e-06, eta: 0:06:46, time: 0.962, data_time: 0.013, memory: 18982, decode.loss_ce: 1.1020, decode.acc_seg: 39.6745, loss: 1.1020
2023-02-19 22:28:36,946 - mmseg - INFO - Iter [7700/8000]	lr: 2.258e-06, eta: 0:05:48, time: 0.965, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0589, decode.acc_seg: 41.5634, loss: 1.0589
2023-02-19 22:29:25,093 - mmseg - INFO - Iter [7750/8000]	lr: 1.883e-06, eta: 0:04:50, time: 0.963, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0172, decode.acc_seg: 41.1101, loss: 1.0172
2023-02-19 22:30:13,210 - mmseg - INFO - Iter [7800/8000]	lr: 1.507e-06, eta: 0:03:51, time: 0.962, data_time: 0.013, memory: 18982, decode.loss_ce: 0.9760, decode.acc_seg: 39.1682, loss: 0.9760
2023-02-19 22:31:01,628 - mmseg - INFO - Iter [7850/8000]	lr: 1.132e-06, eta: 0:02:53, time: 0.968, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0792, decode.acc_seg: 41.5424, loss: 1.0792
2023-02-19 22:31:49,728 - mmseg - INFO - Iter [7900/8000]	lr: 7.575e-07, eta: 0:01:55, time: 0.962, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0970, decode.acc_seg: 40.0256, loss: 1.0970
2023-02-19 22:32:38,073 - mmseg - INFO - Iter [7950/8000]	lr: 3.825e-07, eta: 0:00:57, time: 0.967, data_time: 0.014, memory: 18982, decode.loss_ce: 1.0790, decode.acc_seg: 39.8085, loss: 1.0790
2023-02-19 22:33:26,308 - mmseg - INFO - Saving checkpoint at 8000 iterations
2023-02-19 22:33:28,334 - mmseg - INFO - Exp name: maskclip_plus_r50_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 22:33:28,334 - mmseg - INFO - Iter [8000/8000]	lr: 7.500e-09, eta: 0:00:00, time: 1.005, data_time: 0.013, memory: 18982, decode.loss_ce: 1.0548, decode.acc_seg: 39.0512, loss: 1.0548
2023-02-19 22:56:20,127 - mmseg - INFO - per class results:
2023-02-19 22:56:20,131 - mmseg - INFO - 
+-------------+-------+-------+-------+
|    Class    |  IoU  |  Acc  |  Prec |
+-------------+-------+-------+-------+
|  aeroplane  | 40.96 | 86.54 | 43.74 |
|     bag     | 12.26 | 30.72 | 16.94 |
|     bed     |  4.73 | 20.56 |  5.78 |
|  bedclothes | 14.05 | 33.86 | 19.36 |
|    bench    |  3.69 | 27.85 |  4.08 |
|   bicycle   | 45.17 | 84.76 | 49.16 |
|     bird    | 43.27 | 76.64 | 49.84 |
|     boat    | 16.95 | 65.43 | 18.62 |
|     book    |  6.17 |  6.65 | 46.02 |
|    bottle   | 46.37 | 64.68 | 62.09 |
|   building  | 15.49 | 16.08 | 80.82 |
|     bus     | 65.57 | 77.79 | 80.66 |
|   cabinet   |  17.6 | 25.33 |  36.6 |
|     car     | 59.23 |  77.7 | 71.36 |
|     cat     | 61.26 | 69.91 | 83.21 |
|   ceiling   | 26.94 | 46.18 | 39.27 |
|    chair    | 22.41 | 27.73 | 53.85 |
|    cloth    |  3.94 |  8.03 |  7.19 |
|   computer  |  1.86 | 80.01 |  1.87 |
|     cow     | 21.93 | 47.82 | 28.83 |
|     cup     | 11.83 | 22.22 |  20.2 |
|   curtain   | 16.14 | 47.52 | 19.64 |
|     dog     | 54.01 | 64.06 |  77.5 |
|     door    |  8.05 | 51.23 |  8.72 |
|    fence    | 12.71 | 58.75 | 13.96 |
|    floor    | 29.79 | 45.91 | 45.89 |
|    flower   | 16.35 | 38.21 | 22.23 |
|     food    | 15.52 | 74.45 |  16.4 |
|    grass    | 42.76 | 47.32 |  81.6 |
|    ground   |  5.52 |  5.87 | 48.01 |
|    horse    | 28.85 | 83.28 | 30.62 |
|   keyboard  | 34.95 | 56.37 | 47.91 |
|    light    |  6.11 |  20.6 |  8.0  |
|  motorbike  | 50.81 | 67.68 | 67.09 |
|   mountain  | 17.09 |  37.3 | 23.98 |
|    mouse    |  0.02 |  15.6 |  0.02 |
|    person   | 23.82 | 28.36 | 59.81 |
|    plate    |  3.28 |  6.37 |  6.34 |
|   platform  |  5.98 | 25.53 |  7.25 |
| pottedplant | 29.58 | 70.36 | 33.79 |
|     road    | 22.22 |  27.3 | 54.39 |
|     rock    | 14.84 | 42.21 | 18.63 |
|    sheep    |  9.45 | 71.75 |  9.81 |
|   shelves   |  8.26 | 34.73 |  9.77 |
|   sidewalk  |  9.66 |  47.9 | 10.79 |
|     sign    | 23.62 | 46.38 | 32.49 |
|     sky     | 59.34 | 61.99 | 93.26 |
|     snow    | 27.63 | 66.13 | 32.18 |
|     sofa    |  29.8 | 58.68 | 37.72 |
|    table    | 22.58 | 35.56 | 38.23 |
|    track    |  0.02 |  0.04 |  0.03 |
|    train    | 48.78 | 77.61 | 56.77 |
|     tree    | 30.95 | 32.09 |  89.7 |
|    truck    | 10.11 | 36.51 | 12.27 |
|  tvmonitor  | 22.94 |  28.3 | 54.78 |
|     wall    | 10.75 | 11.55 |  61.0 |
|    water    | 33.29 | 39.36 | 68.35 |
|    window   | 13.37 |  33.7 | 18.15 |
|     wood    |  8.96 | 35.49 | 10.71 |
+-------------+-------+-------+-------+
2023-02-19 22:56:20,131 - mmseg - INFO - Summary:
2023-02-19 22:56:20,131 - mmseg - INFO - 
+-------+-------+-------+-------+
|  aAcc |  mIoU |  mAcc | mPrec |
+-------+-------+-------+-------+
| 41.27 | 22.87 | 44.55 | 36.39 |
+-------+-------+-------+-------+
2023-02-19 22:56:20,149 - mmseg - INFO - Exp name: maskclip_plus_r50_segformer_b5_480x480_8k_pascal_context_59.py
2023-02-19 22:56:20,150 - mmseg - INFO - Iter(val) [5104]	aAcc: 0.4127, mIoU: 0.2287, mAcc: 0.4455, mPrec: 0.3639, IoU.aeroplane: 0.4096, IoU.bag: 0.1226, IoU.bed: 0.0473, IoU.bedclothes: 0.1405, IoU.bench: 0.0369, IoU.bicycle: 0.4517, IoU.bird: 0.4327, IoU.boat: 0.1695, IoU.book: 0.0617, IoU.bottle: 0.4637, IoU.building: 0.1549, IoU.bus: 0.6557, IoU.cabinet: 0.1760, IoU.car: 0.5923, IoU.cat: 0.6126, IoU.ceiling: 0.2694, IoU.chair: 0.2241, IoU.cloth: 0.0394, IoU.computer: 0.0186, IoU.cow: 0.2193, IoU.cup: 0.1183, IoU.curtain: 0.1614, IoU.dog: 0.5401, IoU.door: 0.0805, IoU.fence: 0.1271, IoU.floor: 0.2979, IoU.flower: 0.1635, IoU.food: 0.1552, IoU.grass: 0.4276, IoU.ground: 0.0552, IoU.horse: 0.2885, IoU.keyboard: 0.3495, IoU.light: 0.0611, IoU.motorbike: 0.5081, IoU.mountain: 0.1709, IoU.mouse: 0.0002, IoU.person: 0.2382, IoU.plate: 0.0328, IoU.platform: 0.0598, IoU.pottedplant: 0.2958, IoU.road: 0.2222, IoU.rock: 0.1484, IoU.sheep: 0.0945, IoU.shelves: 0.0826, IoU.sidewalk: 0.0966, IoU.sign: 0.2362, IoU.sky: 0.5934, IoU.snow: 0.2763, IoU.sofa: 0.2980, IoU.table: 0.2258, IoU.track: 0.0002, IoU.train: 0.4878, IoU.tree: 0.3095, IoU.truck: 0.1011, IoU.tvmonitor: 0.2294, IoU.wall: 0.1075, IoU.water: 0.3329, IoU.window: 0.1337, IoU.wood: 0.0896, Acc.aeroplane: 0.8654, Acc.bag: 0.3072, Acc.bed: 0.2056, Acc.bedclothes: 0.3386, Acc.bench: 0.2785, Acc.bicycle: 0.8476, Acc.bird: 0.7664, Acc.boat: 0.6543, Acc.book: 0.0665, Acc.bottle: 0.6468, Acc.building: 0.1608, Acc.bus: 0.7779, Acc.cabinet: 0.2533, Acc.car: 0.7770, Acc.cat: 0.6991, Acc.ceiling: 0.4618, Acc.chair: 0.2773, Acc.cloth: 0.0803, Acc.computer: 0.8001, Acc.cow: 0.4782, Acc.cup: 0.2222, Acc.curtain: 0.4752, Acc.dog: 0.6406, Acc.door: 0.5123, Acc.fence: 0.5875, Acc.floor: 0.4591, Acc.flower: 0.3821, Acc.food: 0.7445, Acc.grass: 0.4732, Acc.ground: 0.0587, Acc.horse: 0.8328, Acc.keyboard: 0.5637, Acc.light: 0.2060, Acc.motorbike: 0.6768, Acc.mountain: 0.3730, Acc.mouse: 0.1560, Acc.person: 0.2836, Acc.plate: 0.0637, Acc.platform: 0.2553, Acc.pottedplant: 0.7036, Acc.road: 0.2730, Acc.rock: 0.4221, Acc.sheep: 0.7175, Acc.shelves: 0.3473, Acc.sidewalk: 0.4790, Acc.sign: 0.4638, Acc.sky: 0.6199, Acc.snow: 0.6613, Acc.sofa: 0.5868, Acc.table: 0.3556, Acc.track: 0.0004, Acc.train: 0.7761, Acc.tree: 0.3209, Acc.truck: 0.3651, Acc.tvmonitor: 0.2830, Acc.wall: 0.1155, Acc.water: 0.3936, Acc.window: 0.3370, Acc.wood: 0.3549, Prec.aeroplane: 0.4374, Prec.bag: 0.1694, Prec.bed: 0.0578, Prec.bedclothes: 0.1936, Prec.bench: 0.0408, Prec.bicycle: 0.4916, Prec.bird: 0.4984, Prec.boat: 0.1862, Prec.book: 0.4602, Prec.bottle: 0.6209, Prec.building: 0.8082, Prec.bus: 0.8066, Prec.cabinet: 0.3660, Prec.car: 0.7136, Prec.cat: 0.8321, Prec.ceiling: 0.3927, Prec.chair: 0.5385, Prec.cloth: 0.0719, Prec.computer: 0.0187, Prec.cow: 0.2883, Prec.cup: 0.2020, Prec.curtain: 0.1964, Prec.dog: 0.7750, Prec.door: 0.0872, Prec.fence: 0.1396, Prec.floor: 0.4589, Prec.flower: 0.2223, Prec.food: 0.1640, Prec.grass: 0.8160, Prec.ground: 0.4801, Prec.horse: 0.3062, Prec.keyboard: 0.4791, Prec.light: 0.0800, Prec.motorbike: 0.6709, Prec.mountain: 0.2398, Prec.mouse: 0.0002, Prec.person: 0.5981, Prec.plate: 0.0634, Prec.platform: 0.0725, Prec.pottedplant: 0.3379, Prec.road: 0.5439, Prec.rock: 0.1863, Prec.sheep: 0.0981, Prec.shelves: 0.0977, Prec.sidewalk: 0.1079, Prec.sign: 0.3249, Prec.sky: 0.9326, Prec.snow: 0.3218, Prec.sofa: 0.3772, Prec.table: 0.3823, Prec.track: 0.0003, Prec.train: 0.5677, Prec.tree: 0.8970, Prec.truck: 0.1227, Prec.tvmonitor: 0.5478, Prec.wall: 0.6100, Prec.water: 0.6835, Prec.window: 0.1815, Prec.wood: 0.1071
